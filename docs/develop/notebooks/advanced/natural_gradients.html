
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Natural gradients &#8212; GPflow 2.6.1 documentation</title>
  <script>
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1e1de1a1873e13ef5536" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1e1de1a1873e13ef5536" rel="stylesheet">

  
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/pydata-custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1e1de1a1873e13ef5536">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/advanced/natural_gradients';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://gpflow.github.io/GPflow/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'develop';
        </script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Ordinal regression" href="ordinal_regression.html" />
    <link rel="prev" title="Multi-output Gaussian processes in GPflow" href="multioutput.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../../index.html">

  
  
  
  
  
  
  

  
    <img src="../../_static/gpflow_logo.svg" class="logo__image only-light" alt="Logo image">
    <img src="../../_static/gpflow_logo.svg" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                <li class="nav-item">
                    <a class="nav-link" href="../../intro.html">
                        Introduction
                    </a>
                </li>
                

                <li class="nav-item">
                    <a class="nav-link" href="../../manual.html">
                        GPflow manual
                    </a>
                </li>
                

                <li class="nav-item">
                    <a class="nav-link" href="../intro_to_gpflow2.html">
                        GPflow with TensorFlow 2
                    </a>
                </li>
                

                <li class="nav-item current active">
                    <a class="nav-link" href="../../notebooks_file.html">
                        Notebooks
                    </a>
                </li>
                

                <li class="nav-item">
                    <a class="nav-link" href="../../derivations.html">
                        Derivations
                    </a>
                </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                <li class="nav-item">
                    <a class="nav-link" href="../../api/gpflow/index.html">
                        API reference
                    </a>
                </li>
                

                <li class="nav-item">
                    <a class="nav-link" href="../../bibliography.html">
                        Bibliography
                    </a>
                </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      <div class="navbar-end-item navbar-end__search-button-container">
        
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
      </div>
      
      <div class="navbar-end-item">
        <div class="version-switcher__container dropdown">
    <button type="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-toggle="dropdown">
        develop  <!-- this text may get changed later by javascript -->
        <span class="caret"></span>
    </button>
    <div class="version-switcher__menu dropdown-menu list-group-flush py-0">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
</div>
      </div>
      
    </div>
  </div>


  
  <div class="search-button-container--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
  </div>

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                <li class="nav-item">
                    <a class="nav-link" href="../../intro.html">
                        Introduction
                    </a>
                </li>
                

                <li class="nav-item">
                    <a class="nav-link" href="../../manual.html">
                        GPflow manual
                    </a>
                </li>
                

                <li class="nav-item">
                    <a class="nav-link" href="../intro_to_gpflow2.html">
                        GPflow with TensorFlow 2
                    </a>
                </li>
                

                <li class="nav-item current active">
                    <a class="nav-link" href="../../notebooks_file.html">
                        Notebooks
                    </a>
                </li>
                

                <li class="nav-item">
                    <a class="nav-link" href="../../derivations.html">
                        Derivations
                    </a>
                </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                <li class="nav-item">
                    <a class="nav-link" href="../../api/gpflow/index.html">
                        API reference
                    </a>
                </li>
                

                <li class="nav-item">
                    <a class="nav-link" href="../../bibliography.html">
                        Bibliography
                    </a>
                </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <div class="version-switcher__container dropdown">
    <button type="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-toggle="dropdown">
        develop  <!-- this text may get changed later by javascript -->
        <span class="caret"></span>
    </button>
    <div class="version-switcher__menu dropdown-menu list-group-flush py-0">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
</div>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Section navigation">
  <p class="bd-links__title" role="heading" aria-level="1">
    Section Navigation
  </p>
  <div class="bd-toc-item navbar-nav">
    <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../basics/GPLVM.html">Bayesian Gaussian process latent variable model (Bayesian GPLVM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/classification.html">Basic (binary) GP classification model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/monitoring.html">Monitoring Optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/regression.html">Basic (Gaussian likelihood) GP regression model</a></li>
</ul>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="changepoints.html">Change points</a></li>
<li class="toctree-l1"><a class="reference internal" href="convolutional.html">Convolutional Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="coregionalisation.html">A simple demonstration of coregionalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="fast_predictions.html">Faster predictions by caching</a></li>
<li class="toctree-l1"><a class="reference internal" href="gps_for_big_data.html">Stochastic Variational Inference for scalability with SVGP</a></li>
<li class="toctree-l1"><a class="reference internal" href="heteroskedastic.html">Heteroskedastic Likelihood and Multi-Latent GP</a></li>
<li class="toctree-l1"><a class="reference internal" href="kernels.html">Manipulating kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">MCMC (Markov Chain Monte Carlo)</a></li>
<li class="toctree-l1"><a class="reference internal" href="multiclass_classification.html">Multiclass classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="multioutput.html">Multi-output Gaussian processes in GPflow</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Natural gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="ordinal_regression.html">Ordinal regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="variational_fourier_features.html">Variational Fourier Features in the GPflow framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="varying_noise.html">Gaussian process regression with varying output noise</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tailor/external-mean-function.html">Custom mean functions: metalearning with GPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tailor/gp_nn.html">Mixing TensorFlow models with GPflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tailor/kernel_design.html">Kernel Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tailor/mixture_density_network.html">Mixture Density Networks in GPflow</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../theory/FITCvsVFE.html">Comparing FITC approximation to VFE approximation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../theory/SGPR_notes.html">Derivation of SGPR equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../theory/Sanity_check.html">Sanity checking when model behaviours should overlap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../theory/cglb.html">Conjugate Gradient Lower Bound</a></li>
<li class="toctree-l1"><a class="reference internal" href="../theory/upper_bound.html">Discussion of the GP marginal likelihood upper bound</a></li>
<li class="toctree-l1"><a class="reference internal" href="../theory/vgp_notes.html">Derivation of VGP equations</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../understanding/models.html">Manipulating GPflow models</a></li>
</ul>

  </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

      </div>
      <main class="bd-main">
        
        
        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                
            </div>
            
            
            <article class="bd-article" role="main">
              
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<section id="Natural-gradients">
<h1>Natural gradients<a class="headerlink" href="#Natural-gradients" title="Permalink to this heading">#</a></h1>
<p>This notebook shows some basic usage of the natural gradient optimizer, both on its own and in combination with Adam optimizer.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">import</span> <span class="nn">gpflow</span>
<span class="kn">from</span> <span class="nn">gpflow</span> <span class="kn">import</span> <span class="n">set_trainable</span>
<span class="kn">from</span> <span class="nn">gpflow.ci_utils</span> <span class="kn">import</span> <span class="n">reduce_in_tests</span>
<span class="kn">from</span> <span class="nn">gpflow.models</span> <span class="kn">import</span> <span class="n">GPR</span><span class="p">,</span> <span class="n">SGPR</span><span class="p">,</span> <span class="n">SVGP</span><span class="p">,</span> <span class="n">VGP</span>
<span class="kn">from</span> <span class="nn">gpflow.optimizers</span> <span class="kn">import</span> <span class="n">NaturalGradient</span>
<span class="kn">from</span> <span class="nn">gpflow.optimizers.natgrad</span> <span class="kn">import</span> <span class="n">XiSqrtMeanVar</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">precision</span> 4

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># inducing points</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">inducing_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">D</span><span class="p">))</span>
<span class="n">adam_learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="n">reduce_in_tests</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">autotune</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2022-10-10 08:53:52.981391: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-10-10 08:53:53.116645: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcudart.so.11.0&#39;; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-10-10 08:53:53.116670: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-10-10 08:53:53.146129: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-10-10 08:53:53.805276: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer.so.7&#39;; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-10-10 08:53:53.805345: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer_plugin.so.7&#39;; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-10-10 08:53:53.805354: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
/home/circleci/project/gpflow/experimental/utils.py:42: UserWarning: You&#39;re calling gpflow.experimental.check_shapes.decorator.check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.
  warn(
/home/circleci/project/gpflow/experimental/utils.py:42: UserWarning: You&#39;re calling gpflow.experimental.check_shapes.inheritance.inherit_check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.
  warn(
2022-10-10 08:53:57.282830: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcuda.so.1&#39;; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-10-10 08:53:57.282858: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)
2022-10-10 08:53:57.282878: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (6c5e0e3edda0): /proc/driver/nvidia/version does not exist
2022-10-10 08:53:57.283121: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
</pre></div></div>
</div>
<section id="VGP-is-a-GPR">
<h2>VGP is a GPR<a class="headerlink" href="#VGP-is-a-GPR" title="Permalink to this heading">#</a></h2>
<p>The following section demonstrates how natural gradients can turn VGP into GPR <em>in a single step, if the likelihood is Gaussian</em>.</p>
<p>Let’s start by first creating a standard GPR model with Gaussian likelihood:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpr</span> <span class="o">=</span> <span class="n">GPR</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">Matern52</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/home/circleci/project/gpflow/experimental/utils.py:42: UserWarning: You&#39;re calling gpflow.experimental.check_shapes.checker.ShapeChecker.__init__ which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.
  warn(
</pre></div></div>
</div>
<p>The log marginal likelihood of the exact GP model is:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpr</span><span class="o">.</span><span class="n">log_marginal_likelihood</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-139.5640
</pre></div></div>
</div>
<p>Now we will create an approximate model which approximates the true posterior via a variational Gaussian distribution.We initialize the distribution to be zero mean and unit variance.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vgp</span> <span class="o">=</span> <span class="n">VGP</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span>
    <span class="n">kernel</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(),</span>
    <span class="n">likelihood</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">likelihoods</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(),</span>
<span class="p">)</span>
<span class="c1"># (Note that GPflow&#39;s NaturalGradient optimizer does not implement diagonal covariance parametrization, i.e., it does not work for `q_diag=True`.)</span>
</pre></div>
</div>
</div>
<p>The log marginal likelihood lower bound (evidence lower bound or ELBO) of the approximate GP model is:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vgp</span><span class="o">.</span><span class="n">elbo</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-426.9017
</pre></div></div>
</div>
<p>Obviously, our initial guess for the variational distribution is not correct, which results in a lower bound to the likelihood of the exact GPR model. We can optimize the variational parameters in order to get a tighter bound.</p>
<p>In fact, we only need to take <strong>one step</strong> in the natural gradient direction to recover the exact posterior:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">natgrad_opt</span> <span class="o">=</span> <span class="n">NaturalGradient</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">variational_params</span> <span class="o">=</span> <span class="p">[(</span><span class="n">vgp</span><span class="o">.</span><span class="n">q_mu</span><span class="p">,</span> <span class="n">vgp</span><span class="o">.</span><span class="n">q_sqrt</span><span class="p">)]</span>
<span class="n">natgrad_opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">vgp</span><span class="o">.</span><span class="n">training_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">variational_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The ELBO of the approximate GP model after a single NatGrad step:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vgp</span><span class="o">.</span><span class="n">elbo</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-139.5641
</pre></div></div>
</div>
</section>
<section id="Optimize-both-variational-parameters-and-kernel-hyperparameters-together">
<h2>Optimize both variational parameters and kernel hyperparameters together<a class="headerlink" href="#Optimize-both-variational-parameters-and-kernel-hyperparameters-together" title="Permalink to this heading">#</a></h2>
<p>In the Gaussian likelihood case we can iterate between an Adam update for the hyperparameters and a NatGrad update for the variational parameters. That way, we achieve optimization of hyperparameters as if the model were a GPR.</p>
<p>The trick is to forbid Adam from updating the variational parameters by setting them to not trainable.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Stop Adam from optimizing the variational parameters</span>
<span class="n">set_trainable</span><span class="p">(</span><span class="n">vgp</span><span class="o">.</span><span class="n">q_mu</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">set_trainable</span><span class="p">(</span><span class="n">vgp</span><span class="o">.</span><span class="n">q_sqrt</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="n">adam_opt_for_vgp</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">adam_learning_rate</span><span class="p">)</span>
<span class="n">adam_opt_for_gpr</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">adam_learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">adam_opt_for_gpr</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
        <span class="n">gpr</span><span class="o">.</span><span class="n">training_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">gpr</span><span class="o">.</span><span class="n">trainable_variables</span>
    <span class="p">)</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">gpr</span><span class="o">.</span><span class="n">log_marginal_likelihood</span><span class="p">()</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPR with Adam: iteration </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2"> likelihood </span><span class="si">{</span><span class="n">likelihood</span><span class="si">:</span><span class="s2">.04f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
GPR with Adam: iteration 1 likelihood -139.2548
GPR with Adam: iteration 2 likelihood -138.9456
GPR with Adam: iteration 3 likelihood -138.6363
GPR with Adam: iteration 4 likelihood -138.3270
GPR with Adam: iteration 5 likelihood -138.0177
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">natgrad_opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">vgp</span><span class="o">.</span><span class="n">training_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">variational_params</span><span class="p">)</span>
    <span class="n">adam_opt_for_vgp</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
        <span class="n">vgp</span><span class="o">.</span><span class="n">training_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">vgp</span><span class="o">.</span><span class="n">trainable_variables</span>
    <span class="p">)</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">vgp</span><span class="o">.</span><span class="n">elbo</span><span class="p">()</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;VGP with NaturalGradient and Adam: iteration </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2"> likelihood </span><span class="si">{</span><span class="n">likelihood</span><span class="si">:</span><span class="s2">.04f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
VGP with NaturalGradient and Adam: iteration 1 likelihood -139.2581
VGP with NaturalGradient and Adam: iteration 2 likelihood -138.9489
VGP with NaturalGradient and Adam: iteration 3 likelihood -138.6397
VGP with NaturalGradient and Adam: iteration 4 likelihood -138.3305
VGP with NaturalGradient and Adam: iteration 5 likelihood -138.0213
</pre></div></div>
</div>
<p>Compare GPR and VGP lengthscales after optimization:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPR lengthscales = </span><span class="si">{</span><span class="n">gpr</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">lengthscales</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s2">.04f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;VGP lengthscales = </span><span class="si">{</span><span class="n">vgp</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">lengthscales</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s2">.04f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
GPR lengthscales = 0.9687
VGP lengthscales = 0.9687
</pre></div></div>
</div>
</section>
<section id="Natural-gradients-also-work-for-the-sparse-model">
<h2>Natural gradients also work for the sparse model<a class="headerlink" href="#Natural-gradients-also-work-for-the-sparse-model" title="Permalink to this heading">#</a></h2>
<p>Similarly, natural gradients turn SVGP into SGPR in the Gaussian likelihood case. We can again combine natural gradients with Adam to update both variational parameters and hyperparameters too. Here we’ll just do a single natural step demonstration.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svgp</span> <span class="o">=</span> <span class="n">SVGP</span><span class="p">(</span>
    <span class="n">kernel</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(),</span>
    <span class="n">likelihood</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">likelihoods</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(),</span>
    <span class="n">inducing_variable</span><span class="o">=</span><span class="n">inducing_variable</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">sgpr</span> <span class="o">=</span> <span class="n">SGPR</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(),</span> <span class="n">inducing_variable</span><span class="o">=</span><span class="n">inducing_variable</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">svgp</span><span class="p">,</span> <span class="n">sgpr</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">variance</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Analytically optimal sparse model ELBO:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sgpr</span><span class="o">.</span><span class="n">elbo</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-227.9840
</pre></div></div>
</div>
<p>SVGP ELBO before natural gradient step:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svgp</span><span class="o">.</span><span class="n">elbo</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-3326.8429
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">variational_params</span> <span class="o">=</span> <span class="p">[(</span><span class="n">svgp</span><span class="o">.</span><span class="n">q_mu</span><span class="p">,</span> <span class="n">svgp</span><span class="o">.</span><span class="n">q_sqrt</span><span class="p">)]</span>

<span class="n">natgrad_opt</span> <span class="o">=</span> <span class="n">NaturalGradient</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">natgrad_opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
    <span class="n">svgp</span><span class="o">.</span><span class="n">training_loss_closure</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">var_list</span><span class="o">=</span><span class="n">variational_params</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>SVGP ELBO after a single natural gradient step:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svgp</span><span class="o">.</span><span class="n">elbo</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-227.9840
</pre></div></div>
</div>
</section>
<section id="Minibatches">
<h2>Minibatches<a class="headerlink" href="#Minibatches" title="Permalink to this heading">#</a></h2>
<p>A crucial property of the natural gradient method is that it still works with minibatches. In practice though, we need to use a smaller gamma.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">natgrad_opt</span> <span class="o">=</span> <span class="n">NaturalGradient</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">data_minibatch</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">autotune</span><span class="p">)</span>
    <span class="o">.</span><span class="n">repeat</span><span class="p">()</span>
    <span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">data_minibatch_it</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">data_minibatch</span><span class="p">)</span>


<span class="n">svgp_objective</span> <span class="o">=</span> <span class="n">svgp</span><span class="o">.</span><span class="n">training_loss_closure</span><span class="p">(</span><span class="n">data_minibatch_it</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reduce_in_tests</span><span class="p">(</span><span class="mi">100</span><span class="p">)):</span>
    <span class="n">natgrad_opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">svgp_objective</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">variational_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Minibatch SVGP ELBO after NatGrad optimization:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
    <span class="p">[</span><span class="n">svgp</span><span class="o">.</span><span class="n">elbo</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">data_minibatch_it</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reduce_in_tests</span><span class="p">(</span><span class="mi">100</span><span class="p">))]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-136.1650
</pre></div></div>
</div>
</section>
<section id="Comparison-with-ordinary-gradients-in-the-conjugate-case">
<h2>Comparison with ordinary gradients in the conjugate case<a class="headerlink" href="#Comparison-with-ordinary-gradients-in-the-conjugate-case" title="Permalink to this heading">#</a></h2>
<p>(Take home message: natural gradients are always better)</p>
<p>Compared to SVGP with ordinary gradients with minibatches, the natural gradient optimizer is much faster in the Gaussian case.</p>
<p>Here we’ll do hyperparameter learning together with optimization of the variational parameters, comparing the interleaved natural gradient approach and the one using ordinary gradients for the hyperparameters and variational parameters jointly.</p>
<p><strong>NOTE:</strong> Again we need to compromise for smaller gamma value, which we’ll keep <em>fixed</em> during the optimization.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svgp_ordinary</span> <span class="o">=</span> <span class="n">SVGP</span><span class="p">(</span>
    <span class="n">kernel</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(),</span>
    <span class="n">likelihood</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">likelihoods</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(),</span>
    <span class="n">inducing_variable</span><span class="o">=</span><span class="n">inducing_variable</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">svgp_natgrad</span> <span class="o">=</span> <span class="n">SVGP</span><span class="p">(</span>
    <span class="n">kernel</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(),</span>
    <span class="n">likelihood</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">likelihoods</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(),</span>
    <span class="n">inducing_variable</span><span class="o">=</span><span class="n">inducing_variable</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># ordinary gradients with Adam for SVGP</span>
<span class="n">ordinary_adam_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">adam_learning_rate</span><span class="p">)</span>

<span class="c1"># NatGrads and Adam for SVGP</span>
<span class="c1"># Stop Adam from optimizing the variational parameters</span>
<span class="n">set_trainable</span><span class="p">(</span><span class="n">svgp_natgrad</span><span class="o">.</span><span class="n">q_mu</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">set_trainable</span><span class="p">(</span><span class="n">svgp_natgrad</span><span class="o">.</span><span class="n">q_sqrt</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="c1"># Create the optimize_tensors for SVGP</span>
<span class="n">natgrad_adam_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">adam_learning_rate</span><span class="p">)</span>

<span class="n">natgrad_opt</span> <span class="o">=</span> <span class="n">NaturalGradient</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">variational_params</span> <span class="o">=</span> <span class="p">[(</span><span class="n">svgp_natgrad</span><span class="o">.</span><span class="n">q_mu</span><span class="p">,</span> <span class="n">svgp_natgrad</span><span class="o">.</span><span class="n">q_sqrt</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<p>Let’s optimize the models:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_minibatch</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">autotune</span><span class="p">)</span>
    <span class="o">.</span><span class="n">repeat</span><span class="p">()</span>
    <span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">data_minibatch_it</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">data_minibatch</span><span class="p">)</span>


<span class="n">svgp_ordinary_loss</span> <span class="o">=</span> <span class="n">svgp_ordinary</span><span class="o">.</span><span class="n">training_loss_closure</span><span class="p">(</span><span class="n">data_minibatch_it</span><span class="p">)</span>
<span class="n">svgp_natgrad_loss</span> <span class="o">=</span> <span class="n">svgp_natgrad</span><span class="o">.</span><span class="n">training_loss_closure</span><span class="p">(</span><span class="n">data_minibatch_it</span><span class="p">)</span>


<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reduce_in_tests</span><span class="p">(</span><span class="mi">100</span><span class="p">)):</span>
    <span class="n">ordinary_adam_opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
        <span class="n">svgp_ordinary_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">svgp_ordinary</span><span class="o">.</span><span class="n">trainable_variables</span>
    <span class="p">)</span>


<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reduce_in_tests</span><span class="p">(</span><span class="mi">100</span><span class="p">)):</span>
    <span class="n">natgrad_adam_opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
        <span class="n">svgp_natgrad_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">svgp_natgrad</span><span class="o">.</span><span class="n">trainable_variables</span>
    <span class="p">)</span>
    <span class="n">natgrad_opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">svgp_natgrad_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">variational_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>SVGP ELBO after ordinary <code class="docutils literal notranslate"><span class="pre">Adam</span></code> optimization:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">svgp_ordinary</span><span class="o">.</span><span class="n">elbo</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">data_minibatch_it</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reduce_in_tests</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-82.7712
</pre></div></div>
</div>
<p>SVGP ELBO after <code class="docutils literal notranslate"><span class="pre">NaturalGradient</span></code> and <code class="docutils literal notranslate"><span class="pre">Adam</span></code> optimization:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">svgp_natgrad</span><span class="o">.</span><span class="n">elbo</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">data_minibatch_it</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reduce_in_tests</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-73.0862
</pre></div></div>
</div>
</section>
<section id="Comparison-with-ordinary-gradients-in-the-non-conjugate-case">
<h2>Comparison with ordinary gradients in the non-conjugate case<a class="headerlink" href="#Comparison-with-ordinary-gradients-in-the-non-conjugate-case" title="Permalink to this heading">#</a></h2>
<section id="Binary-classification">
<h3>Binary classification<a class="headerlink" href="#Binary-classification" title="Permalink to this heading">#</a></h3>
<p>(Take home message: natural gradients are usually better)</p>
<p>We can use natural gradients even when the likelihood isn’t Gaussian. It isn’t guaranteed to be better, but it usually is better in practical situations.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_binary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">vgp_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_binary</span><span class="p">)</span>

<span class="n">vgp_bernoulli</span> <span class="o">=</span> <span class="n">VGP</span><span class="p">(</span>
    <span class="n">vgp_data</span><span class="p">,</span>
    <span class="n">kernel</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(),</span>
    <span class="n">likelihood</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">likelihoods</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(),</span>
<span class="p">)</span>
<span class="n">vgp_bernoulli_natgrad</span> <span class="o">=</span> <span class="n">VGP</span><span class="p">(</span>
    <span class="n">vgp_data</span><span class="p">,</span>
    <span class="n">kernel</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(),</span>
    <span class="n">likelihood</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">likelihoods</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(),</span>
<span class="p">)</span>

<span class="c1"># ordinary gradients with Adam for VGP with Bernoulli likelihood</span>
<span class="n">adam_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">adam_learning_rate</span><span class="p">)</span>

<span class="c1"># NatGrads and Adam for VGP with Bernoulli likelihood</span>
<span class="c1"># Stop Adam from optimizing the variational parameters</span>
<span class="n">set_trainable</span><span class="p">(</span><span class="n">vgp_bernoulli_natgrad</span><span class="o">.</span><span class="n">q_mu</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">set_trainable</span><span class="p">(</span><span class="n">vgp_bernoulli_natgrad</span><span class="o">.</span><span class="n">q_sqrt</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="c1"># Create the optimize_tensors for VGP with natural gradients</span>
<span class="n">natgrad_adam_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">adam_learning_rate</span><span class="p">)</span>
<span class="n">natgrad_opt</span> <span class="o">=</span> <span class="n">NaturalGradient</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">variational_params</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="n">vgp_bernoulli_natgrad</span><span class="o">.</span><span class="n">q_mu</span><span class="p">,</span> <span class="n">vgp_bernoulli_natgrad</span><span class="o">.</span><span class="n">q_sqrt</span><span class="p">)</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Optimize vgp_bernoulli</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reduce_in_tests</span><span class="p">(</span><span class="mi">100</span><span class="p">)):</span>
    <span class="n">adam_opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
        <span class="n">vgp_bernoulli</span><span class="o">.</span><span class="n">training_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">vgp_bernoulli</span><span class="o">.</span><span class="n">trainable_variables</span>
    <span class="p">)</span>

<span class="c1"># Optimize vgp_bernoulli_natgrad</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reduce_in_tests</span><span class="p">(</span><span class="mi">100</span><span class="p">)):</span>
    <span class="n">adam_opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
        <span class="n">vgp_bernoulli_natgrad</span><span class="o">.</span><span class="n">training_loss</span><span class="p">,</span>
        <span class="n">var_list</span><span class="o">=</span><span class="n">vgp_bernoulli_natgrad</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">natgrad_opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
        <span class="n">vgp_bernoulli_natgrad</span><span class="o">.</span><span class="n">training_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">variational_params</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<p>VGP ELBO after ordinary <code class="docutils literal notranslate"><span class="pre">Adam</span></code> optimization:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vgp_bernoulli</span><span class="o">.</span><span class="n">elbo</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-144.2140
</pre></div></div>
</div>
<p>VGP ELBO after <code class="docutils literal notranslate"><span class="pre">NaturalGradient</span></code> + <code class="docutils literal notranslate"><span class="pre">Adam</span></code> optimization:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vgp_bernoulli_natgrad</span><span class="o">.</span><span class="n">elbo</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-142.8380
</pre></div></div>
</div>
<p>We can also choose to run natural gradients in another parameterization. The sensible choice is the model parameters (q_mu, q_sqrt), which is already in GPflow.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vgp_bernoulli_natgrads_xi</span> <span class="o">=</span> <span class="n">VGP</span><span class="p">(</span>
    <span class="n">vgp_data</span><span class="p">,</span>
    <span class="n">kernel</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(),</span>
    <span class="n">likelihood</span><span class="o">=</span><span class="n">gpflow</span><span class="o">.</span><span class="n">likelihoods</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(),</span>
<span class="p">)</span>

<span class="c1"># Stop Adam from optimizing the variational parameters</span>
<span class="n">set_trainable</span><span class="p">(</span><span class="n">vgp_bernoulli_natgrads_xi</span><span class="o">.</span><span class="n">q_mu</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">set_trainable</span><span class="p">(</span><span class="n">vgp_bernoulli_natgrads_xi</span><span class="o">.</span><span class="n">q_sqrt</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="c1"># Create the optimize_tensors for VGP with Bernoulli likelihood</span>
<span class="n">adam_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">adam_learning_rate</span><span class="p">)</span>
<span class="n">natgrad_opt</span> <span class="o">=</span> <span class="n">NaturalGradient</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">variational_params</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span>
        <span class="n">vgp_bernoulli_natgrads_xi</span><span class="o">.</span><span class="n">q_mu</span><span class="p">,</span>
        <span class="n">vgp_bernoulli_natgrads_xi</span><span class="o">.</span><span class="n">q_sqrt</span><span class="p">,</span>
        <span class="n">XiSqrtMeanVar</span><span class="p">(),</span>
    <span class="p">)</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Optimize vgp_bernoulli_natgrads_xi</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reduce_in_tests</span><span class="p">(</span><span class="mi">100</span><span class="p">)):</span>
    <span class="n">adam_opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
        <span class="n">vgp_bernoulli_natgrads_xi</span><span class="o">.</span><span class="n">training_loss</span><span class="p">,</span>
        <span class="n">var_list</span><span class="o">=</span><span class="n">vgp_bernoulli_natgrads_xi</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">natgrad_opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
        <span class="n">vgp_bernoulli_natgrads_xi</span><span class="o">.</span><span class="n">training_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">variational_params</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
</pre></div></div>
</div>
<p>VGP ELBO after <code class="docutils literal notranslate"><span class="pre">NaturalGradient</span></code> with <code class="docutils literal notranslate"><span class="pre">XiSqrtMeanVar</span></code> + <code class="docutils literal notranslate"><span class="pre">Adam</span></code> optimization:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vgp_bernoulli_natgrads_xi</span><span class="o">.</span><span class="n">elbo</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-143.0989
</pre></div></div>
</div>
<p>With sufficiently small steps, it shouldn’t make a difference which transform is used, but for large steps this can make a difference in practice.</p>
</section>
</section>
</section>


            </article>
            
            
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#VGP-is-a-GPR">
   VGP is a GPR
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#Optimize-both-variational-parameters-and-kernel-hyperparameters-together">
   Optimize both variational parameters and kernel hyperparameters together
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#Natural-gradients-also-work-for-the-sparse-model">
   Natural gradients also work for the sparse model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#Minibatches">
   Minibatches
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#Comparison-with-ordinary-gradients-in-the-conjugate-case">
   Comparison with ordinary gradients in the conjugate case
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#Comparison-with-ordinary-gradients-in-the-non-conjugate-case">
   Comparison with ordinary gradients in the non-conjugate case
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#Binary-classification">
     Binary classification
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
</div>

<div class="toc-item">
  
<div id="searchbox"></div>
</div>

<div class="toc-item">
  
</div>

<div class="toc-item">
  
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
          </div>
        </footer>
        
      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1e1de1a1873e13ef5536"></script>

  <footer class="bd-footer"><div class="bd-footer__inner container">
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2022, The GPflow Contributors.<br>

</p>

  </div>
  
  <div class="footer-item">
    
<p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.2.3.<br>
</p>

  </div>
  
</div>
  </footer>
  </body>
</html>