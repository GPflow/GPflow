{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import gpflow as gp\n",
    "\n",
    "import gpflow.multioutput.kernels as mk\n",
    "import gpflow.multioutput.features as mfa\n",
    "\n",
    "from gpflow import settings\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Computation time increases when I create gpflow objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Bad example:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(2, 4):\n",
    "    kernel = gpflow.kernels.RBF(1)  # It is a gpflow object with tf.Variables inside\n",
    "    x = np.random.randn(n, 1)\n",
    "    kxx = kernel.K(x)  # It is a tensor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember, we operate on a tensorflow graph!**\n",
    "\n",
    "Everytime we create (build and compile) a new gpflow object, although we override the kernel variable, we keep adding more tensors to the graph and just change the reference to them.\n",
    "\n",
    "So, unnecessary expanding the graph slows down your computation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Good example (imagine running this code snippet in `ipython` again and again):_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(2, 4):\n",
    "    gpflow.reset_default_graph_and_session()\n",
    "    kernel = gpflow.kernels.RBF(1)\n",
    "    x = np.random.randn(n, 1)\n",
    "    kxx = kernel.K(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(2, 4):\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        with tf.Session(graph=graph).as_default():\n",
    "            kernel = gpflow.kernels.RBF(1)\n",
    "            x = np.random.randn(n, 1)\n",
    "            kxx = kernel.K(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How can I reuse a model on different data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(2, 1)\n",
    "y = np.random.randn(2, 1)\n",
    "kernel = gpflow.kernels.RBF(1)\n",
    "model = gpflow.models.GPR(x, y, kernel)\n",
    "\n",
    "x_new = np.random.randn(100, 1)\n",
    "y_new = np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the loglikelihood of the model on different data. Note, we haven't changed the original model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-146.19026836342385"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor = model.X.parameter_tensor\n",
    "y_tensor = model.Y.parameter_tensor\n",
    "model.compute_log_likelihood(feed_dict={x_tensor: x_new, y_tensor: y_new})  # we can still probe the model with the old data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same by updating (permanently) the value of the dataholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-146.19026836342385"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.X = x_new\n",
    "model.Y = y_new\n",
    "model.compute_log_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. I would like to use external TensorFlow tensors and pass them to a GPflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass tensorflow tensors for any non-trainable parameter of the gpflow objects like DataHolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-196.77394755910805"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = gpflow.kernels.RBF(1)\n",
    "likelihood = gpflow.likelihoods.Gaussian()\n",
    "\n",
    "x_tensor = tf.random_normal((100, 1), dtype=settings.float_type)\n",
    "y_tensor = tf.random_normal((100, 1), dtype=settings.float_type)\n",
    "z = np.random.randn(10, 1)\n",
    "\n",
    "model = gpflow.models.SVGP(x_tensor, y_tensor, kern=kernel, likelihood=likelihood, Z=z)\n",
    "model.compute_log_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use TensorFlow variables for trainable objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.Variable(np.random.randn(10, 1))\n",
    "model = gpflow.models.SVGP(x_tensor, y_tensor, kern=kernel, likelihood=likelihood, Z=z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you have to initialize them manually, before iteracting with a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-187.27162747308242"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = gpflow.get_default_session()\n",
    "session.run(z.initializer)\n",
    "model.compute_log_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. I would like to share parameters between GPflow objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want to impose a hard-coded strtucture to the model.\n",
    "\n",
    "For example, we have a multi-output model where some output dimensions share the same kernel and others don't.\n",
    "\n",
    "Unfortunately we cannot do this after the kernel object is compiled. We have to do it at build time and then manually compile the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gpflow.decors.defer_build():\n",
    "    kernels = [gpflow.kernels.RBF(1) for _ in range(3)]\n",
    "    mo_kernels = mk.SeparateMixedMok(kernels, W=np.random.randn(3, 4))\n",
    "    mo_kernels.kernels[0].lengthscales = mo_kernels.kernels[1].lengthscales\n",
    "    mo_kernels.compile()\n",
    "\n",
    "assert mo_kernels.kernels[0].lengthscales == mo_kernels.kernels[1].lengthscales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. When I try to read parameter values, I'm getting stale values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100, 1)\n",
    "y = np.random.randn(100, 1)\n",
    "\n",
    "kernel = gpflow.kernels.RBF(1)\n",
    "model = gpflow.models.GPR(x, y, kernel)\n",
    "optimizer = gpflow.training.AdamOptimizer()\n",
    "optimizer_tensor = optimizer.make_optimize_tensor(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial value before optimisation is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.kern.lengthscales.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call one step of the optimization and check the new value of the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpflow.get_default_session().run(optimizer_tensor)\n",
    "model.kern.lengthscales.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After optimization you would expect that the parameters have been updated, but they haven't. The trick is that the `value` property returns a cached numpy value of a parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the value of the optimized parameter via the `read_value()` method, specifying the correct `session`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0006322363201534"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.kern.lengthscales.read_value(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can `anchor()` your model to the session after the optimisation step. After that your parameters' cache have been updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.00063224)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.anchor(session)\n",
    "model.kern.lengthscales.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Optimizing again my model slows down the computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.random.randn(100, 1)\n",
    "y = np.random.randn(100, 1)\n",
    "model = gpflow.models.GPR(x, y, kernel)\n",
    "\n",
    "optimizer = gpflow.training.AdamOptimizer()\n",
    "\n",
    "optimizer.minimize(model, maxiter=2)\n",
    "\n",
    "# Do something with the model\n",
    "\n",
    "optimizer.minimize(model, maxiter=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `minimize()` call creates a bunch of optimization tensors. Calling `minimize()` again causes the same issue pointed in question (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct way of doing it without polluting your graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gpflow.kernels.RBF(1)\n",
    "x = np.random.randn(100, 1)\n",
    "y = np.random.randn(100, 1)\n",
    "model = gpflow.models.GPR(x, y, kernel)\n",
    "\n",
    "optimizer = gpflow.training.AdamOptimizer()\n",
    "optimizer_tensor = optimizer.make_optimize_tensor(model)\n",
    "session = gpflow.get_default_session()\n",
    "for _ in range(2):\n",
    "    session.run(optimizer_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to **anchor** your model to the session after optimisation steps. Then you can continue working with your model.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.anchor(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you need to optimize it again, you can reuse the same optimizer tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    session.run(optimizer_tensor)\n",
    "\n",
    "model.anchor(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I save/load a GPflow model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gpflow.kernels.RBF(1)\n",
    "x = np.random.randn(100, 1)\n",
    "y = np.random.randn(100, 1)\n",
    "model = gpflow.models.GPR(x, y, kernel)\n",
    "\n",
    "\n",
    "filename = \"/tmp/gpr.gpflow\"\n",
    "path = Path(filename)\n",
    "if path.exists():\n",
    "    path.unlink()\n",
    "saver = gpflow.saver.Saver()\n",
    "saver.save(filename, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the model back into different graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as graph, tf.Session().as_default():\n",
    "    model_copy = saver.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can load the model into the same session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_for_loading = gpflow.saver.SaverContext(autocompile=False)\n",
    "model_copy = saver.load(filename, context=ctx_for_loading)\n",
    "model_copy.clear()\n",
    "model_copy.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the former and the latter approach is in tensorflow name scopes which are used for naming variables. The former approach replicates the instance of the tensorflow objects (which already exist in the original graph), so we need to load it in a new graph.\n",
    "The latter uses different name scopes for the variables so we can dump the model in the same graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
