{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import itertools\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"\"\n",
    "import numpy as np\n",
    "import gpflow\n",
    "import gpflow.training.monitor as mon\n",
    "import numbers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(10000, 1) * 10\n",
    "Y = np.sin(X) + np.random.randn(*X.shape)\n",
    "Xt = np.random.rand(10000, 1) * 10\n",
    "Yt = np.sin(Xt) + np.random.randn(*Xt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: `gpflow.training.monitor`\n",
    "In this notebook we'll demo how to use `gpflow.training.monitor` for logging the optimisation of a GPflow model. The example should cover pretty much all use cases.\n",
    "\n",
    "## Creating the GPflow model\n",
    "We first create the GPflow model. Under the hood, GPflow gives a unique name to each model which is used to name the Variables it creates in the TensorFlow graph containing a random identifier. This is useful in interactive sessions, where people may create a few models, to prevent variables with the same name conflicting. However, when loading the model, we need to make sure that the names of all the variables are exactly the same as in the checkpoint. This is why we pass `name=\"SVGP\"` to the model constructor, and why we use `gpflow.defer_build()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gpflow.defer_build():\n",
    "    kernel = gpflow.kernels.RBF(1)\n",
    "    likelihood = gpflow.likelihoods.Gaussian()\n",
    "    Z = np.linspace(0, 10, 5)[:, None]\n",
    "    m = gpflow.models.SVGP(X, Y, kern=kernel, likelihood=likelihood, Z=Z, minibatch_size=100, name=\"SVGP\")\n",
    "    m.likelihood.variance = 0.01\n",
    "\n",
    "m.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.compute_log_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the optimisation\n",
    "Next we need to set up the optimisation process. `gpflow.training.monitor` provides classes that manage the optimsation, and perform certain logging tasks. In this example, we want to:\n",
    "- log certain scalar parameters in TensorBoard,\n",
    "- log the full optimisation objective (log marginal likelihood bound) periodically, even though we optimise with minibatches,\n",
    "- store a backup of the optimisation process periodically,\n",
    "- log performance for a test set periodically.\n",
    "\n",
    "Because of the integration with TensorFlow ways of storing and logging, we will need to perform a few TensorFlow manipulations outside of GPflow as well.\n",
    "\n",
    "We start by creating the `global_step` variable. This is not strictly required by TensorFlow optimisers, but they do all have support for it. Its purpose is to track how many optimisation steps have occurred. It is useful to keep this in a TensorFlow variable as this allows it to be restored together with all the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "m.enquire_session().run(global_step.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the optimiser action. `make_optimize_action` also creates the optimisation tensor, which is added to the computational graph. Later, the saver will store the whole graph, and so can also restore the exact optimiser state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = gpflow.train.AdamOptimizer(0.01).make_optimize_action(m, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating actions for keeping track of the optimisation\n",
    "We now create an instance of `FileWriter`, which will save the TensorBoard logs to a file. This object needs to be shared between all `gpflow_monitor.TensorBoard` objects, if they are to write to the same path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a filewriter for summaries\n",
    "fw = tf.summary.FileWriter('./model_tensorboard', m.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the TensorFlow side is set up, we can focus on the `monitor` part. Each part of the monitoring process is taken care of by an `Action`. Each `Action` is something that needs to be run periodically during the optimisation. The first and second parameters of all actions are a generator returning times (either in iterations or time) of when the action needs to be run. The second determines whether a number of iterations (`Trigger.ITER`), or an amount of wall-clock time (`Trigger.TOTAL_TIME`) triggers the `Action` to be run. The following `Action`s are run once in every 10 or 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_lml = mon.PrintTimings(itertools.count(), mon.Trigger.ITER, single_line=True, global_step=global_step)\n",
    "sleep = mon.SleepAction(itertools.count(), mon.Trigger.ITER, 0.01)\n",
    "saver = mon.StoreSession(itertools.count(step=10), mon.Trigger.ITER, m.enquire_session(),\n",
    "                         hist_path=\"./monitor-saves/checkpoint\", global_step=global_step)\n",
    "tensorboard = mon.ModelTensorBoard(itertools.count(step=10), mon.Trigger.ITER, m, fw, global_step=global_step)\n",
    "lml_tensorboard = mon.LmlTensorBoard(itertools.count(step=100), mon.Trigger.ITER, m, fw, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisation step is also encapsulated in an `Action`, in this case the `adam` variable which we created earlier. We place all actions in a list in the order that they should be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actions = [adam, print_lml, tensorboard, lml_tensorboard, saver, sleep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom `Action`s\n",
    "We may also want to perfom certain tasks that do not have pre-defined `Action` classes. For example, we may want to compute the performance on a test set. Here we create such a class by extending `ModelTensorBoard` to log the testing benchmarks in addition to all the scalar parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TestTensorBoard(mon.ModelTensorBoard):\n",
    "    def __init__(self, sequence, trigger: mon.Trigger, model, file_writer, Xt, Yt, *, global_step=global_step):\n",
    "        super().__init__(sequence, trigger, model, file_writer, global_step=global_step)\n",
    "        self.Xt = Xt\n",
    "        self.Yt = Yt\n",
    "        self._full_test_err = tf.placeholder(gpflow.settings.tf_float, shape=())\n",
    "        self._full_test_nlpp = tf.placeholder(gpflow.settings.tf_float, shape=())\n",
    "        self.summary = tf.summary.merge([tf.summary.scalar(\"test_rmse\", self._full_test_err),\n",
    "                                         tf.summary.scalar(\"test_nlpp\", self._full_test_nlpp)])\n",
    "\n",
    "    def run(self, ctx):\n",
    "        minibatch_size = 100\n",
    "        preds = np.vstack([self.model.predict_y(Xt[mb * minibatch_size:(mb + 1) * minibatch_size, :])[0]\n",
    "                            for mb in range(-(-len(Xt) // minibatch_size))])\n",
    "        test_err = np.mean((Yt - preds) ** 2.0)**0.5\n",
    "        summary, step = self.model.enquire_session().run([self.summary, self.global_step],\n",
    "                                      feed_dict={self._full_test_err: test_err,\n",
    "                                                 self._full_test_nlpp: 0.0})\n",
    "        self.file_writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add the custom `TestTensorBoard` to the list which will be run later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actions.append(TestTensorBoard(itertools.count(step=100), mon.Trigger.ITER, m, fw, Xt, Yt, global_step=global_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the optimisation\n",
    "We finally get to running the optimisation. The second time this is run, the session should be restored from a checkpoint created by `StoreSession`. This is important to ensure that the optimiser starts off from _exactly_ the same state as that it left. If this is not done correctly, models may start diverging after loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gpflow.actions.Loop(actions, stop=500)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
