{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mv310/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "import gpflow\n",
    "import gpflow.training.monitor as mon\n",
    "import numbers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "X = np.random.rand(10000, 1) * 10\n",
    "Y = np.sin(X) + np.random.randn(*X.shape)\n",
    "Xt = np.random.rand(10000, 1) * 10\n",
    "Yt = np.sin(X) + np.random.randn(*X.shape)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: `gpflow.training.monitor`\n",
    "In this notebook we'll demo how to use `gpflow.training.monitor` for logging the optimisation of a GPflow model. The example should cover pretty much all use cases.\n",
    "\n",
    "## Creating the GPflow model\n",
    "We first create the GPflow model as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gpflow.models.SVGP(X, Y, gpflow.kernels.RBF(1), gpflow.likelihoods.Gaussian(),\n",
    "                       Z=np.linspace(0, 10, 5)[:, None],\n",
    "                       minibatch_size=100)\n",
    "m.likelihood.variance = 0.01\n",
    "m.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1113603.4585405581"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.compute_log_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "m.enquire_session().run(global_step.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_adam(model, lr, iterations, callback=None, **kwargs):\n",
    "    adam = gpflow.train.AdamOptimizer(lr).make_optimize_action(model, **kwargs)\n",
    "    print_lml = gpflow.actions.Condition(lambda ctx: ctx.iteration % 10 == 0,\n",
    "                                         mon.PrintAction(model, \"lml\", single_line=True))\n",
    "    sleep = mon.SleepAction(0.03)\n",
    "    actions = [adam, print_lml, sleep] + ([] if callback is None else callback)\n",
    "    gpflow.actions.Loop(actions, stop=iterations)()\n",
    "    model.anchor(model.enquire_session())  # <-- Why is this needed?\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1288392330170609\n",
      "0.4410382729838602ikelihood -1183103.1783\n",
      "0.7528173869941384likelihood -955596.9000\n",
      "1.0663029860006645likelihood -780802.4905\n",
      "1.3763707610196434likelihood -728184.1209\n",
      "1.6855529780150391likelihood -625976.0261\n",
      "1.9955675569945015likelihood -474838.8423\n",
      "2.3045703640091233likelihood -361842.4505\n",
      "2.6133613510173745likelihood -307700.7837\n",
      "2.9251141659915447likelihood -319473.2593\n",
      "lml: iteration 90 likelihood -277542.3179\r"
     ]
    }
   ],
   "source": [
    "loop = run_adam(m, 0.01, 100, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445588.432593929\n"
     ]
    }
   ],
   "source": [
    "print(loop[2].watcher._start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compute_log_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443034.3406466"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the optimisation\n",
    "Next we need to set up the optimisation process. `gpflow_monitor` provides classes that manage the optimsation, and perform certain logging tasks. In this example, we want to:\n",
    "- log certain scalar parameters in TensorBoard\n",
    "- log the full optimisation objective (log marginal likelihood bound) periodically, even though we optimise with minibatches\n",
    "- store a backup of the optimisation process periodically\n",
    "- log performance for a test set periodically\n",
    "\n",
    "Because of the integration with TensorFlow ways of storing and logging, we will need to perform a few TensorFlow manipulations outside of GPflow as well.\n",
    "\n",
    "We start by creating the `global_step` variable. This is not strictly required by TensorFlow optimisers, but they do all have support for it. Its purpose is to track how many optimisation steps have occurred. It is useful to keep this in a TensorFlow variable as this allows it to be restored together with all the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "m.enquire_session().run(global_step.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an instance of `FileWriter`, which will save the TensorBoard logs to a file. This object needs to be shared between all `gpflow_monitor.TensorBoard` objects, if they are to write to the same path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fw = tf.summary.FileWriter(os.path.join(\"./results/test/tensorboard/\"), m.enquire_session().graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the TensorFlow side is set up, we can focus on the `gpflow_monitor` part. The optimsation is taken care of by the `ManagedOptimisation` class. This will run the training loop. The `ManagedOptimisation` object will also take care of running `Task`s.\n",
    "\n",
    "Each `Task` is something that needs to be run periodically during the optimisation. The first and second parameters of all tasks are a generator returning times (either in iterations or time) of when the `Task` needs to be run. The second determines whether a number of iterations (`Trigger.ITER`), an amount of time spent optimising (`Trigger.OPTIMISATION_TIME`), or the wall-clock time (`Trigger.TOTAL_TIME`) triggers the `Task` to be run. The following `Task`s are run once in every 100 or 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name full lml is illegal; using full_lml instead.\n"
     ]
    }
   ],
   "source": [
    "opt_method = ManagedOptimisation(m, gpflow.train.AdamOptimizer(0.01), global_step)\n",
    "opt_method.tasks += [\n",
    "    PrintTimings((x * 100 for x in itertools.count()), Trigger.ITER),\n",
    "    ModelTensorBoard((x * 100 for x in itertools.count()), Trigger.ITER, m, fw),\n",
    "    LmlTensorBoard((x * 1000 for x in itertools.count()), Trigger.ITER, m, fw, verbose=False),\n",
    "    StoreSession((x * 1000 for x in itertools.count()), Trigger.ITER, m.enquire_session(), \"./results/test/checkpoint\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to perfom certain tasks that do not have pre-defined `Task` classes. For example, computing the performance on a test set. Here we create such a class by extending `ModelTensorBoard` to log the testing benchmarks in addition to all the scalar parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestTensorBoard(ModelTensorBoard):\n",
    "    def __init__(self, sequence, trigger: Trigger, model, file_writer, Xt, Yt):\n",
    "        super().__init__(sequence, trigger, model, file_writer)\n",
    "        self.Xt = Xt\n",
    "        self.Yt = Yt\n",
    "        self._full_test_err = tf.placeholder(gpflow.settings.tf_float, shape=())\n",
    "        self._full_test_nlpp = tf.placeholder(gpflow.settings.tf_float, shape=())\n",
    "\n",
    "        self.summary = tf.summary.merge([tf.summary.scalar(\"test_rmse\", self._full_test_err),\n",
    "                                         tf.summary.scalar(\"test_nlpp\", self._full_test_nlpp)])\n",
    "\n",
    "    def _event_handler(self, manager):\n",
    "        minibatch_size = 100\n",
    "        preds = np.vstack([m.predict_y(Xt[mb * minibatch_size:(mb + 1) * minibatch_size, :])[0]\n",
    "                            for mb in range(-(-len(Xt) // minibatch_size))])\n",
    "        test_err = np.mean((Yt - preds) ** 2.0)**0.5\n",
    "        summary, step = m.enquire_session().run([self.summary, global_step],\n",
    "                                      feed_dict={self._full_test_err: test_err,\n",
    "                                                 self._full_test_nlpp: 0.0})\n",
    "        self.file_writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add it to the task list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt_method.tasks.append(TestTensorBoard((x * 1000 for x in itertools.count()), Trigger.ITER, m, fw, Xt, Yt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the optimisation\n",
    "We finally get to running the optimisation. The second time this is run, the session should be restored from a checkpoint created by `StoreSession`. To confirm this, we print out the first value in all TensorFlow tensors. This includes any values used by the optimiser. This is important to ensure that the optimiser starts off from _exactly_ the same state as that it left. If this is not done correctly, models may start diverging after loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.54132327263575086,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.54132327263575086,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -4.6002665251585171,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.89999998,\n",
       " 0.99900001,\n",
       " 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = m.enquire_session()\n",
    "[u[1] if isinstance(u[1], numbers.Number) else u[1].flatten()[0]  for u in sorted([(v.name, sess.run(v)) for v in tf.global_variables()], key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 1:\t2.16 optimisation iter/s\t2.16 total iter/s\t0.00 last iter/sFull lml: -1198200.924550 (-1.20e+06)\n",
      "1000, 1000:\t454.39 optimisation iter/s\t325.05 total iter/s\t576.21 last iter/sFull lml: -30515.580038 (-3.05e+04)\n",
      "2000, 2000:\t506.20 optimisation iter/s\t367.32 total iter/s\t570.79 last iter/sFull lml: -17402.941729 (-1.74e+04)\n",
      "3000, 3000:\t527.07 optimisation iter/s\t384.49 total iter/s\t573.41 last iter/sFull lml: -15024.233225 (-1.50e+04)\n"
     ]
    }
   ],
   "source": [
    "opt_method.minimize(maxiter=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we print the optimised variables for comparison on the next run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[u[1] if isinstance(u[1], numbers.Number) else u[1].flatten()[0]  for u in sorted([(v.name, sess.run(v)) for v in tf.global_variables()], key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
