{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic GP Classification model\n",
    "\n",
    "\n",
    "This script shows how to build a GP classification model using variational inference. We first look at a one-dimensional example, and then show how this can be adapted when the input space is 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D example\n",
    "\n",
    "First of all, let's have a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('data/classif_1D_X.csv').reshape(-1, 1)\n",
    "Y = np.genfromtxt('data/classif_1D_Y.csv').reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X, Y, 'C3x', ms=8, mew=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminders on the GP classification\n",
    "\n",
    "The simplest GP classification model consists in using a `Bernouilli` likelihood. The details of the generative model are as follow:\n",
    "\n",
    "__1. Define the latent GP:__ we start from a gaussian process $f \\sim \\mathcal{N}(0, k)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the kernel and covariance matrix\n",
    "k = gpflow.kernels.Matern52(1, variance=20.)\n",
    "x_grid = np.linspace(0, 6, 200).reshape(-1, 1)\n",
    "K = k.compute_K_symm(x_grid)\n",
    "\n",
    "# sample from a multivariate normal\n",
    "L = np.linalg.cholesky(K)\n",
    "f_grid = np.dot(L, np.random.RandomState(6).randn(200, 5))\n",
    "plt.plot(x_grid, f_grid, 'C0', linewidth=1)\n",
    "plt.plot(x_grid, f_grid[:, 1], 'C0', linewidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Squash them to $[0, 1]$:__ the samples of the GP are mapped to $[0, 1]$ using the logistic inverse link function: $g(x) = \\frac{\\exp(f(x))}{1 + \\exp(f(x))}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_grid = np.exp(f_grid) / (1 + np.exp(f_grid))\n",
    "plt.plot(x_grid, p_grid, 'C1', linewidth=1)\n",
    "plt.plot(x_grid, p_grid[:, 1], 'C1', linewidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Sample from a Bernouilli:__ for each observation point $X_i$, the class label $Y_i \\in \\{0, 1\\}$ is generated by sampling from a Bernouilli distribution $Y_i \\sim \\mathcal{B}(g(X_i))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select some input locations\n",
    "ind = np.random.randint(0, 200, (30,))\n",
    "X_gen = x_grid[ind]\n",
    "\n",
    "# evaluate probability and get bernoulli draws\n",
    "p = p_grid[ind, 1:2]\n",
    "Y_gen = np.random.binomial(1, p)\n",
    "\n",
    "#plot\n",
    "plt.plot(x_grid, p_grid[:, 1], 'C1', linewidth=2)\n",
    "plt.plot(X_gen, p, 'C1o', ms=6)\n",
    "plt.plot(X_gen, Y_gen, 'C3x', ms=8, mew=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation with GPflow\n",
    "\n",
    "For the model described above, the posterior $f(x)|Y$ (say $p$) is not gaussian anymore and does not have a closed form expression. A common approach is then to look for the best approximation of this posterior by a tractable distribution (say $q$) such as a Gaussian distribution. In variational inference, the quality of an approximation is measured by the KL divergence between $KL[q || p]$. For more details on this model, see Nickisch et al (2008).\n",
    "\n",
    "The inference problem is thus turned into an optimisation problem: finding the best parametres for $q$. In our case, we introduce $U \\sim \\mathcal{N}(q_\\mu, q_\\Sigma)$ we choose $q$ to have the same distribution as $f | f(X) = U$. The parameters $q_\\mu$ and $q_\\Sigma$ can be seen as parametres of $q$, which can be optimised in order to minimise  $KL[q || p]$. \n",
    "\n",
    "This variational inference model is called `VGP` in GPflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gpflow.models.VGP(X, Y,\n",
    "                      likelihood=gpflow.likelihoods.Bernoulli(),\n",
    "                      kern=gpflow.kernels.Matern52(1))\n",
    "\n",
    "o = gpflow.train.ScipyOptimizer()\n",
    "o.minimize(m);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the result of the optimisation with `print(m)` or `m.as_pandas_table()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.as_pandas_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this table, the first two lines are associated to the kernel parametres, and the last two correspond to the variational parameters. Note that in practice, $q_\\Sigma$ is actually parametrised by its lower-triangular square root $q_\\Sigma = q_{sqrt} q_{sqrt}^T$ in oder to ensure it's positive definiteness.\n",
    "\n",
    "For more details on how to handle models in GPflow (getting/setting parameters, fixing some of them during optim, using priors...), we refer the reader to the [notebook on models](../understanding/models.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Finally, we will see how to use model predictions to plot the resulting model. We will replicate the figures of the generative model above, but using the approximate posterior distribution given by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# bubble fill the predictions\n",
    "mu, var = m.predict_f(x_grid)\n",
    "plt.fill_between(x_grid.flatten(),\n",
    "                 (mu + 2 * np.sqrt(var)).flatten(),\n",
    "                 (mu - 2 * np.sqrt(var)).flatten(),\n",
    "                 alpha=0.3, color='C0')\n",
    "    \n",
    "# plot samples\n",
    "samples = m.predict_f_samples(x_grid, 10).squeeze().T\n",
    "plt.plot(x_grid, samples, 'C0', lw=1)\n",
    "    \n",
    "# plot p-samples\n",
    "p = np.exp(samples) / (1. + np.exp(samples))\n",
    "plt.plot(x_grid, p, 'C1', lw=1)\n",
    "\n",
    "# plot data\n",
    "plt.plot(X, Y, 'C3x', ms=8, mew=2)\n",
    "plt.ylim((-3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D example\n",
    "\n",
    "In this section we will use the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('data/banana_X_train', delimiter=',')\n",
    "Y = np.loadtxt('data/banana_Y_train', delimiter=',').reshape(-1,1)\n",
    "mask = Y[:, 0]==1\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(X[mask, 0], X[mask, 1], 'oC0', mew=0, alpha=0.5)\n",
    "plt.plot(X[np.logical_not(mask), 0], X[np.logical_not(mask), 1], 'oC1', mew=0, alpha=0.5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model definition is the same as above, the only important difference is that we now specify that the kernel operates over a 2D input space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gpflow.models.VGP(X, Y,\n",
    "                      kern=gpflow.kernels.RBF(2),\n",
    "                      likelihood=gpflow.likelihoods.Bernoulli())\n",
    "\n",
    "opt = gpflow.train.ScipyOptimizer()\n",
    "opt.minimize(m, maxiter=50) # in practice, the optimisation needs around 250 iterations to converge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the predicted decision boundary between the two classes. To do so, we can equivalently plot the contourlines $E[f(x)|Y]=0$, or $E[g(f(x))|Y]=.5$. We will do the later since it allows us to discover a the function `predict_y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(-3, 3, 40)\n",
    "xx, yy = np.meshgrid(x_grid, x_grid)\n",
    "Xplot = np.vstack((xx.flatten(),yy.flatten())).T\n",
    "\n",
    "p = m.predict_y(Xplot)[0]\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(X[mask, 0], X[mask, 1], 'oC0', mew=0, alpha=0.5)\n",
    "plt.plot(X[np.logical_not(mask), 0], X[np.logical_not(mask), 1], 'oC1', mew=0, alpha=0.5);\n",
    "\n",
    "plt.contour(xx, yy, p.reshape(*xx.shape), [0.5], colors='k', linewidths=1.8, zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further reading\n",
    "\n",
    "There are dedicated notebooks giving more details on how to manipulate [models](../understanding/models.ipynb) and [kernels](../advanced/kernels.ipynb).\n",
    "\n",
    "This notebook only covers very basic classification models. You may also be interested in:\n",
    "  * [Sparse models](many_points.ipynb). The models above have one inducing variable $U_i$ per observation point $X_i$, which does not scale to large datasets. SVGP (Sparse Variational GP) is an efficient alternative where the variables $U_i$ are defined at some inducing input locations $Z_i$ that can also be optimized.\n",
    "  * [Exact inference](../advanced/MCMC.ipynb). We have seen that variational inference provides an approximation to the posterior. GPflow also supports exact inference using MCMC methods, and the kernel parameters can also be assigned prior distribution in order to avoid point estimates.\n",
    "  \n",
    "# References\n",
    "\n",
    "[1] Nickisch, Hannes, and Carl Edward Rasmussen. \"Approximations for binary Gaussian process classification.\" Journal of Machine Learning Research 9.Oct (2008): 2035-2078."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
