{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPflow manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document can be used to get familiarised with GPflow. We've split up the material in four different categories.\n",
    "\n",
    "## Basics\n",
    "\n",
    "The basics notebooks cover the elementairy uses of GPflow, where we show how to use GPflow for your basic datasets with existing models.\n",
    "\n",
    "  - In [regression.ipynb](basics/regression.ipynb) and [classification.ipynb](basics/classification.ipynb) we show how to use GPflow to fit a simple regression and classification models (Rasmussen and Williams, 2006)\n",
    "  - In [gplvm.ipynb](basics/GPLVM.ipynb) we cover the unsupervised case, and showcase GPflow's Bayesian GPLVM implementation (Titsias and Lawrence, 2010).\n",
    "  - When you're dealing with a large datasets (over 1k), you want to resort to Sparse methods. In [GPs for big data.ipynb](basics/many_points.ipynb)  we show how to use GPflow's Sparse Variational GP (SVGP) model (Hensman et al., 2013; 2015)\n",
    "\n",
    "In each of these notebooks we go over the data format, model setup, model optimisation and predict options.\n",
    "\n",
    "## Understanding\n",
    "\n",
    "This section covers the building blocks of GPflow from a implementation perspective, and show how the different modules interact as a whole.\n",
    "  - [Architecture](understanding/architecture.ipynb)  **[TODO]**\n",
    "  - [Utilities](understanding/utilities.ipynb): expectations, multi-output, conditionals, kullbach leibers (KL), log-densities, features and quadrature  **[TODO]**\n",
    "  - [Manipulating models](understanding/models.ipynb)  **[TODO]**\n",
    "  - [Handling TensorFlow graph and sessions](understanding/tf_graphs_and_sessions.ipynb).\n",
    "\n",
    "    \n",
    "## Advanced Needs\n",
    "\n",
    "GPflow also allows for more complex features and models:\n",
    "\n",
    "*Models:*\n",
    "  - [MCMC](advanced/mcmc.ipynb)  **[TODO]**\n",
    "  - [ordinal regression](advanced/ordinal_regression.ipynb)\n",
    "  - [multi-class classification](advanced/multiclass_classification.ipynb)\n",
    "  - [multi-outputs and coregionalisation](advanced/multioutputs_and_coregionalisation.ipynb)  **[TODO]**\n",
    "    - [multi-outputs and coregionalisation](advanced/multioutputs_and_coregionalisation.ipynb)  **[TODO]**\n",
    "  - [advanced many-points](advanced/advanced_many_points.ipynb)  **[TODO]**\n",
    "  - [Manipulating kernels](advanced/kernels.ipynb) shows what covariances are included in the library, and how they can be combined to create new ones.\n",
    "\n",
    "*Features:*\n",
    "  - [natural gradients](advanced/natural_gradients.ipynb)\n",
    "  - [optimisers](advanced/optimisation.ipynb)  **[TODO]**\n",
    "  - [settings](advanced/settings.ipynb): Adjust jitter (for inversion or Cholesky errors), floating point precision, parallelism, and more.\n",
    "  - [Monitoring parameter optimisations](advanced/monitoring.ipynb): Sending things to TensorBoard, (re)storing checkpoints, and more.\n",
    "\n",
    "## Tailored models\n",
    "\n",
    "In this section, we show how GPflow's utilities and codebase can be used to build new probabilistic models.\n",
    "These can be seen as complete examples.\n",
    "  - [kernel design](tailor/kernel_design.ipynb) Shows how to implement a covariance function that is not available by default in GPflow. For this example, we looked at the Brownian motion covariance.\n",
    "  - [likelihood design](tailor/likelihood_design.ipynb) **[TODO]**\n",
    "  - [Latent variable models](tailor/models_with_latent_variables.ipynb) **[TODO]**\n",
    "  - [Updating models with new data](tailor/updating_models_with_new_data.ipynb) **[TODO]**\n",
    "  - [External mean functions](tailor/external_mean_functions.ipynb) **[TODO]**\n",
    "  - [Mixture density network](tailor/mixture_density_network.ipynb): we show how we can use GPflow's utilities to build a non GP probabilistic model in no time.\n",
    "    \n",
    "\n",
    "### References\n",
    "Carl E Rasmussen and Christopher KI Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.\n",
    "\n",
    "James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian Processes for Big Data. Uncertainty in Artificial Intelligence, 2013.\n",
    "\n",
    "James Hensman, Alexander G de G Matthews, and Zoubin Ghahramani. Scalable variational Gaussian process classification. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, 2015.\n",
    "\n",
    "Titsias, Michalis, and Neil D. Lawrence. Bayesian Gaussian process latent variable model. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. 2010.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
