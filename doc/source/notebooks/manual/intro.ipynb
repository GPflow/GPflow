{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPflow manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document can be used to get familiarised with GPflow. We've split up the material in four different categories.\n",
    "\n",
    "## Basics\n",
    "\n",
    "The basics notebooks cover the elementairy uses of GPflow, where we show how to use GPflow for your basic datasets with existing models.\n",
    "\n",
    "  - In [regression.ipynb](basics/regression.ipynb) and [classification.ipynb](basics/classification.ipynb) **[TODO, Nicolas]** we show how to use GPflow to fit a simple regression and classification models (Rasmussen and Williams, 2006)\n",
    "  - In [gplvm.ipynb](basics/GPLVM.ipynb) we cover the unsupervised case, and showcase GPflow's Bayesian GPLVM implementation (Titsias and Lawrence, 2010).\n",
    "  - When you're dealing with a large datasets (over 1k), you want to resort to Sparse methods. In [many_points.ipynb](basics/many_points.ipynb)  **[TODO]** we show how to use GPflow's Sparse Variational GP (SVGP) model (Hensman et al., 2013; 2015)\n",
    "\n",
    "In each of these notebooks we go over the data format, model setup, model optimisation and predict options.\n",
    "\n",
    "## Understanding\n",
    "\n",
    "This section covers the building blocks of GPflow from a implementation perspective, and show how the different modules interact as a whole.\n",
    "  - [Architecture](understanding/architecture.ipynb)  **[TODO]**\n",
    "  - [Utilities](understanding/utilities.ipynb): expectations, multi-output, conditionals, kullbach leibers (KL), log-densities, features and quadrature  **[TODO]**\n",
    "  - [Handling TensorFlow graph and sessions](understanding/tf_graphs_and_sessions.ipynb)  **[TODO]**\n",
    "\n",
    "    \n",
    "## Advanced Needs\n",
    "\n",
    "GPflow also allows for more complex features and models:\n",
    "\n",
    "*Models:*\n",
    "  - [MCMC](advanced/mcmc.ipynb)  **[TODO]**\n",
    "  - [ordinal regression](advanced/ordinal_regression.ipynb)  **[TODO]**\n",
    "  - [multi-class classification](advanced/multiclass_classification.ipynb)  **[TODO]**\n",
    "  - [multi-outputs and coregionalisation](advanced/multioutputs_and_coregionalisation.ipynb)  **[TODO]**\n",
    "  - [advanced many-points](advanced/advanced_many_points.ipynb)  **[TODO]**\n",
    "  - playing with [kernels](advanced/kernels.ipynb)  **[TODO]**\n",
    "\n",
    "*Features:*\n",
    "  - [natural gradients](advanced/natural_gradients.ipynb)  **[TODO]**\n",
    "  - [optimisers](advanced/optimisation.ipynb)  **[TODO]**\n",
    "  - [settings or rc-config](advanced/settings.ipynb)  **[TODO]**\n",
    "  - [Monitoring parameter optimisations](advanced/monitoring.ipynb) **[TODO]**\n",
    "\n",
    "## Tailored models\n",
    "\n",
    "In this section, we show how GPflow's utilities and codebase can be used to build new probabilistic models.\n",
    "These can be seen as complete examples.\n",
    "  - [kernel design](tailor/kernel_design.ipynb) **[TODO]**\n",
    "  - [likelihood design](tailor/likelihood_design.ipynb) **[TODO]**\n",
    "  - [Latent variable models](tailor/models_with_latent_variables.ipynb) **[TODO]**\n",
    "  - [Updating models with new data](tailor/updating_models_with_new_data.ipynb) **[TODO]**\n",
    "  - [External mean functions](tailor/external_mean_functions.ipynb) **[TODO]**\n",
    "  - [mixture density network](tailor/mixture_density_network.ipynb) **[TODO]**\n",
    "    \n",
    "\n",
    "### References\n",
    "Carl E Rasmussen and Christopher KI Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.\n",
    "\n",
    "James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian Processes for Big Data. Uncertainty in Artificial Intelligence, 2013.\n",
    "\n",
    "James Hensman, Alexander G de G Matthews, and Zoubin Ghahramani. Scalable variational Gaussian process classification. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, 2015.\n",
    "\n",
    "Titsias, Michalis, and Neil D. Lawrence. Bayesian Gaussian process latent variable model. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. 2010.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
