{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPflow manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document can be used to get familiarised with GPflow. We've split up the material in four differnent categories.\n",
    "1. The `basics` notebooks cover the elementairy uses of GPflow, where we show how to use GPflow for your basic datasets with existing models.\n",
    "    - In [regression.ipynb]() and [classification.ipynb]() we show how to use GPflow to fit a simple regression and classification models (Rasmussen and Williams, 2006)\n",
    "    - In [gplvm.ipynb]() we cover the unsupervised case, and showcase GPflow's Bayesian GPLVM implementation (Titsias and Lawrence, 2010).\n",
    "    - When you're dealing with a large datasets (over 1k), you want to resort to Sparse methods. In [many_points.ipynb]() we show how to use GPflow's Sparse Variational GP (SVGP) model (Hensman et al., 2013; 2015)\n",
    "\n",
    "In each of these notebooks we go over the data format, model setup, model optimisation and predict options.\n",
    "\n",
    "2. In `understand` we cover the building blocks of GPflow from a implementation perspective, and show how the different modules interact as a whole.\n",
    "    - Architecture [TODO]()\n",
    "    - Utilities: expectations, multi-output, conditionals, kullbach leibers (KL), log-densities, features and quadrature [TODO]()\n",
    "    - TF graph and sessions [TODO]()\n",
    "    - monitoring\n",
    "    \n",
    "3. In `Advanced` we cover more complex features and models\n",
    "Models:\n",
    "    - MCMC [second part of regression.ipynb and ]\n",
    "    - ordinal regression [ordinal.ipynb]()\n",
    "    - multi-class classification [first part of multiclass.ipynb]\n",
    "    - multi-output and coregionalisation [NEW notebook explain usage and difference: multioutput.ipynb and coreg_demo.ipynb]\n",
    "    - playing with kernels [first part kernels notebook]()\n",
    "Features:\n",
    "    - nat-grads [natural_gradients.ipynb]()\n",
    "    - optimisation [TODO]()\n",
    "    - settings: rc-config [settings.ipynb]()\n",
    "\n",
    "4. In `Tailor` we show how GPflow's utilities and codebase can be used to build new probabilistic models.\n",
    "These can be seen as complete examples.\n",
    "    - kernel design [TODO]()\n",
    "    - likelihood design [TODO]()\n",
    "    - Latent variable models [TODO]()\n",
    "    - Updating models with new data [TODO]()\n",
    "    - External mean functions\n",
    "    - mixture density network\n",
    "    \n",
    "\n",
    "### References\n",
    "Carl E Rasmussen and Christopher KI Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.\n",
    "\n",
    "James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian Processes for Big Data. Uncertainty in Artificial Intelligence, 2013.\n",
    "\n",
    "James Hensman, Alexander G de G Matthews, and Zoubin Ghahramani. Scalable variational Gaussian process classification. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, 2015.\n",
    "\n",
    "Titsias, Michalis, and Neil D. Lawrence. Bayesian Gaussian process latent variable model. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. 2010.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
