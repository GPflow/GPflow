{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural gradients\n",
    "\n",
    "This notebook shows some basic usage of the natural gradient optimizer, both on its own and in combination with Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T13:31:56.149218Z",
     "start_time": "2018-06-13T13:31:55.213980Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from gpflow.ci_utils import ci_niter, ci_range\n",
    "from gpflow.models import VGP, GPR, SGPR, SVGP\n",
    "from gpflow.optimizers import NaturalGradient\n",
    "from gpflow.optimizers.natgrad import XiSqrtMeanVar\n",
    "\n",
    "%matplotlib inline\n",
    "%precision 4\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "N, D = 100, 2\n",
    "batch_size = 50\n",
    "\n",
    "# inducing points\n",
    "M = 10\n",
    "\n",
    "x = np.random.uniform(size=(N, D))\n",
    "y = np.sin(10 * x)\n",
    "\n",
    "data = (x, y)\n",
    "inducing_variable = tf.random.uniform((M, D))\n",
    "adam_learning_rate = 0.01\n",
    "iterations = ci_niter(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGP is a GPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section demonstrates how natural gradients can turn VGP into GPR *in a single step, if the likelihood is Gaussian*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by first creating a standard GPR model with Gaussian likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T09:59:57.799754Z",
     "start_time": "2018-04-30T09:59:57.547423Z"
    }
   },
   "outputs": [],
   "source": [
    "gpr = GPR(data, kernel=gpflow.kernels.Matern52())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of the exact GP model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-231.08993573018935"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpr.log_likelihood().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create an approximate model which approximates the true posterior via a variational Gaussian distribution.<br>We initialize the distribution to be zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T09:59:57.799754Z",
     "start_time": "2018-04-30T09:59:57.547423Z"
    }
   },
   "outputs": [],
   "source": [
    "vgp = VGP(data, kernel=gpflow.kernels.Matern52(), likelihood=gpflow.likelihoods.Gaussian())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of the approximate GP model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-328.84293853435463"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgp.log_likelihood().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, our initial guess for the variational distribution is not correct, which results in a lower bound to the likelihood of the exact GPR model. We can optimize the variational parameters in order to get a tighter bound. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we only need to take **one step** in the natural gradient direction to recover the exact posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T09:59:58.254414Z",
     "start_time": "2018-04-30T09:59:57.864617Z"
    }
   },
   "outputs": [],
   "source": [
    "natgrad_opt = NaturalGradient(gamma=1.0)\n",
    "variational_params = [(vgp.q_mu, vgp.q_sqrt)]\n",
    "natgrad_opt.minimize(lambda: - vgp.log_marginal_likelihood(), var_list=variational_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of the approximate GP model after a single NatGrad step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-231.08999824749554"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgp.log_likelihood().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize both variational parameters and kernel hyperparameters together\n",
    "\n",
    "In the Gaussian likelihood case we can iterate between an Adam update for the hyperparameters and a NatGrad update for the variational parameters. That way, we achieve optimization of hyperparameters as if the model were a GPR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick is to forbid Adam from updating the variational parameters by setting them to not trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Adam from optimizing the variational parameters\n",
    "vgp.q_mu.trainable = False\n",
    "vgp.q_sqrt.trainable = False\n",
    "\n",
    "adam_opt_for_vgp = tf.optimizers.Adam(adam_learning_rate)\n",
    "adam_opt_for_gpr = tf.optimizers.Adam(adam_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPR with Adam: iteration 1 likelihood -230.6706\n",
      "GPR with Adam: iteration 2 likelihood -230.2508\n",
      "GPR with Adam: iteration 3 likelihood -229.8303\n",
      "GPR with Adam: iteration 4 likelihood -229.4093\n",
      "GPR with Adam: iteration 5 likelihood -228.9876\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    adam_opt_for_gpr.minimize(\n",
    "        lambda: - gpr.log_marginal_likelihood(), \n",
    "        var_list=gpr.trainable_variables)\n",
    "    likelihood = gpr.log_likelihood()\n",
    "    tf.print(f'GPR with Adam: iteration {i + 1} likelihood {likelihood:.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGP with NaturalGradient and Adam: iteration 1 likelihood -230.6707\n",
      "VGP with NaturalGradient and Adam: iteration 2 likelihood -230.2508\n",
      "VGP with NaturalGradient and Adam: iteration 3 likelihood -229.8304\n",
      "VGP with NaturalGradient and Adam: iteration 4 likelihood -229.4093\n",
      "VGP with NaturalGradient and Adam: iteration 5 likelihood -228.9877\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    adam_opt_for_vgp.minimize(\n",
    "        lambda: - vgp.log_marginal_likelihood(), \n",
    "        var_list=vgp.trainable_variables)\n",
    "    natgrad_opt.minimize(\n",
    "        lambda: - vgp.log_marginal_likelihood(), \n",
    "        var_list=variational_params)\n",
    "    likelihood = vgp.log_likelihood()\n",
    "    tf.print(f'VGP with NaturalGradient and Adam: iteration {i + 1} likelihood {likelihood:.04f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare GPR and VGP lengthscales after optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T09:59:59.505965Z",
     "start_time": "2018-04-30T09:59:59.503129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPR lengthscales = 0.9686\n",
      "VGP lengthscales = 0.9686\n"
     ]
    }
   ],
   "source": [
    "print(f'GPR lengthscales = {gpr.kernel.lengthscale.numpy():.04f}')\n",
    "print(f'VGP lengthscales = {vgp.kernel.lengthscale.numpy():.04f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural gradients also work for the sparse model\n",
    "Similarly, natural gradients turn SVGP into SGPR in the Gaussian likelihood case. <br>\n",
    "We can again combine natural gradients with Adam to update both variational parameters and hyperparameters too.<br>\n",
    "Here we'll just do a single natural step demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T09:59:59.946016Z",
     "start_time": "2018-04-30T09:59:59.507143Z"
    }
   },
   "outputs": [],
   "source": [
    "svgp = SVGP(kernel=gpflow.kernels.Matern52(), likelihood=gpflow.likelihoods.Gaussian(), inducing_variable=inducing_variable)\n",
    "sgpr = SGPR(data, kernel=gpflow.kernels.Matern52(), inducing_variable=inducing_variable)\n",
    "\n",
    "for model in svgp, sgpr:\n",
    "    model.likelihood.variance.assign(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytically optimal sparse model likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-301.3217357213807"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgpr.log_likelihood().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVGP likelihood before natural gradient step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1404.0804961509486"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svgp.log_likelihood(data).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T10:00:00.691979Z",
     "start_time": "2018-04-30T10:00:00.053823Z"
    }
   },
   "outputs": [],
   "source": [
    "variational_params = [(svgp.q_mu, svgp.q_sqrt)]\n",
    "\n",
    "def svgp_loss_cb():\n",
    "    return - svgp.log_marginal_likelihood(data)\n",
    "\n",
    "natgrad_opt = NaturalGradient(gamma=1.0)\n",
    "natgrad_opt.minimize(svgp_loss_cb, var_list=variational_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVGP likelihood after a single natural gradient step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-315.41174669482126"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svgp.log_likelihood(data).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatches\n",
    "A crucial property of the natural gradient method is that it still works with minibatches.\n",
    "In practice though, we need to use a smaller gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T10:00:01.667254Z",
     "start_time": "2018-04-30T10:00:00.692973Z"
    }
   },
   "outputs": [],
   "source": [
    "natgrad_opt = NaturalGradient(gamma=0.1)\n",
    "\n",
    "data_minibatch = tf.data.Dataset.from_tensor_slices(data).prefetch(N).repeat().shuffle(N).batch(batch_size)\n",
    "data_minibatch_it = iter(data_minibatch)\n",
    "\n",
    "def svgp_stochastic_loss_cb() -> tf.Tensor:\n",
    "    batch = next(data_minibatch_it)\n",
    "    return - svgp.log_marginal_likelihood(batch)\n",
    "\n",
    "for _ in range(ci_niter(100)):\n",
    "    natgrad_opt.minimize(svgp_stochastic_loss_cb, var_list=variational_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch SVGP likelihood after NatGrad optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-166.7657946763955"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([svgp.log_likelihood(next(data_minibatch_it)) for _ in ci_range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with ordinary gradients in the conjugate case\n",
    "\n",
    "##### (Take home message: natural gradients are always better)\n",
    "\n",
    "Compared to SVGP with ordinary gradients with minibatches, the natural gradient optimizer is much faster in the Gaussian case. \n",
    "\n",
    "Here we'll do hyperparameter learning together with optimization of the variational parameters, comparing the interleaved natural gradient approach and the one using ordinary gradients for the hyperparameters and variational parameters jointly.\n",
    "\n",
    "**NOTE:** Again we need to compromise for smaller gamma value, which we'll keep *fixed* during the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T13:32:06.105670Z",
     "start_time": "2018-06-13T13:32:02.550268Z"
    }
   },
   "outputs": [],
   "source": [
    "svgp_ordinary = SVGP(kernel=gpflow.kernels.Matern52(), likelihood=gpflow.likelihoods.Gaussian(), inducing_variable=inducing_variable)\n",
    "svgp_natgrad = SVGP(kernel=gpflow.kernels.Matern52(), likelihood=gpflow.likelihoods.Gaussian(), inducing_variable=inducing_variable)\n",
    "\n",
    "# ordinary gradients with Adam for SVGP\n",
    "ordinary_adam_opt = tf.optimizers.Adam(adam_learning_rate)\n",
    "\n",
    "# NatGrads and Adam for SVGP\n",
    "# Stop Adam from optimizing the variational parameters\n",
    "svgp_natgrad.q_mu.trainable = False\n",
    "svgp_natgrad.q_sqrt.trainable = False\n",
    "\n",
    "# Create the optimize_tensors for SVGP\n",
    "natgrad_adam_opt = tf.optimizers.Adam(adam_learning_rate)\n",
    "\n",
    "natgrad_opt = NaturalGradient(gamma=0.1)\n",
    "variational_params = [(svgp_natgrad.q_mu, svgp_natgrad.q_sqrt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's optimize the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_minibatch = tf.data.Dataset.from_tensor_slices(data).prefetch(N).repeat().shuffle(N).batch(batch_size)\n",
    "data_minibatch_it = iter(data_minibatch)\n",
    "\n",
    "\n",
    "def svgp_ordinary_loss_cb() -> tf.Tensor:\n",
    "    batch = next(data_minibatch_it)\n",
    "    return - svgp_ordinary.log_marginal_likelihood(batch)\n",
    "\n",
    "\n",
    "def svgp_natgrad_loss_cb() -> tf.Tensor:\n",
    "    batch = next(data_minibatch_it)\n",
    "    return - svgp_natgrad.log_marginal_likelihood(batch)\n",
    "\n",
    "\n",
    "for _ in range(ci_niter(100)):\n",
    "    ordinary_adam_opt.minimize(svgp_ordinary_loss_cb, var_list=svgp_ordinary.trainable_variables)\n",
    "\n",
    "\n",
    "for _ in range(ci_niter(100)):\n",
    "    natgrad_adam_opt.minimize(svgp_natgrad_loss_cb, var_list=svgp_natgrad.trainable_variables)\n",
    "    natgrad_opt.minimize(svgp_natgrad_loss_cb, var_list=variational_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVGP likelihood after ordinary `Adam` optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-104.49762723906062"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([svgp_ordinary.log_likelihood(next(data_minibatch_it)) for _ in ci_range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVGP likelihood after `NaturalGradient` and `Adam` optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-102.54594520791255"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([svgp_natgrad.log_likelihood(next(data_minibatch_it)) for _ in ci_range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with ordinary gradients in the non-conjugate case\n",
    "#### Binary classification\n",
    "\n",
    "##### (Take home message: natural gradients are usually better)\n",
    "\n",
    "We can use natural gradients even when the likelihood isn't Gaussian. It isn't guaranteed to be better, but it usually is better in practical situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T13:32:13.397570Z",
     "start_time": "2018-06-13T13:32:10.157801Z"
    }
   },
   "outputs": [],
   "source": [
    "y_binary = np.random.choice([1., -1], size=x.shape)\n",
    "vgp_data = (x, y_binary)\n",
    "\n",
    "vgp_bernoulli = VGP(vgp_data, kernel=gpflow.kernels.Matern52(), likelihood=gpflow.likelihoods.Bernoulli())\n",
    "vgp_bernoulli_natgrad = VGP(vgp_data, kernel=gpflow.kernels.Matern52(), likelihood=gpflow.likelihoods.Bernoulli())\n",
    "\n",
    "# ordinary gradients with Adam for VGP with Bernoulli likelihood\n",
    "adam_opt = tf.optimizers.Adam(adam_learning_rate)\n",
    "\n",
    "# NatGrads and Adam for VGP with Bernoulli likelihood\n",
    "# Stop Adam from optimizing the variational parameters\n",
    "vgp_bernoulli_natgrad.q_mu.trainable = False\n",
    "vgp_bernoulli_natgrad.q_sqrt.trainable = False\n",
    "\n",
    "# Create the optimize_tensors for VGP with natural gradients\n",
    "natgrad_adam_opt = tf.optimizers.Adam(adam_learning_rate)\n",
    "natgrad_opt = NaturalGradient(gamma=0.1)\n",
    "variational_params = [(vgp_bernoulli_natgrad.q_mu, vgp_bernoulli_natgrad.q_sqrt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize vgp_bernoulli\n",
    "for _ in range(ci_niter(100)):\n",
    "    adam_opt.minimize(\n",
    "        lambda: - vgp_bernoulli.log_marginal_likelihood(),\n",
    "        var_list=vgp_bernoulli.trainable_variables)\n",
    "\n",
    "# Optimize vgp_bernoulli_natgrad\n",
    "for _ in range(ci_niter(100)):\n",
    "    adam_opt.minimize(\n",
    "        lambda: - vgp_bernoulli_natgrad.log_marginal_likelihood(),  \n",
    "        var_list=vgp_bernoulli_natgrad.trainable_variables)\n",
    "    natgrad_opt.minimize(\n",
    "            lambda: - vgp_bernoulli_natgrad.log_marginal_likelihood(),\n",
    "            var_list=variational_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGP likelihood after ordinary `Adam` optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-144.21400720445877"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgp_bernoulli.log_likelihood().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGP likelihood after `NaturalGradient` + `Adam` optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-142.83799599076423"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgp_bernoulli_natgrad.log_likelihood().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose to run natural gradients in another parameterization.<br>\n",
    "The sensible choice is the model parameters (q_mu, q_sqrt), which is already in GPflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgp_bernoulli_natgrads_xi = VGP(vgp_data, kernel=gpflow.kernels.Matern52(), likelihood=gpflow.likelihoods.Bernoulli())\n",
    "\n",
    "# Stop Adam from optimizing the variational parameters\n",
    "vgp_bernoulli_natgrads_xi.q_mu.trainable = False\n",
    "vgp_bernoulli_natgrads_xi.q_sqrt.trainable = False\n",
    "\n",
    "# Create the optimize_tensors for VGP with Bernoulli likelihood\n",
    "adam_opt = tf.optimizers.Adam(adam_learning_rate)\n",
    "natgrad_opt = NaturalGradient(gamma=0.01)\n",
    "\n",
    "variational_params = [(vgp_bernoulli_natgrads_xi.q_mu, vgp_bernoulli_natgrads_xi.q_sqrt, XiSqrtMeanVar())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1021 21:53:24.205371 4725601728 backprop.py:980] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    }
   ],
   "source": [
    "# Optimize vgp_bernoulli_natgrads_xi\n",
    "for _ in range(ci_niter(100)):\n",
    "    adam_opt.minimize(\n",
    "        lambda: - vgp_bernoulli_natgrads_xi.log_marginal_likelihood(),                \n",
    "        var_list=vgp_bernoulli_natgrads_xi.trainable_variables)\n",
    "\n",
    "    natgrad_opt.minimize(\n",
    "        lambda: - vgp_bernoulli_natgrads_xi.log_marginal_likelihood(),\n",
    "        var_list=variational_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGP likelihood after `NaturalGradient` with `XiSqrtMeanVar` + `Adam` optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-143.09890480317517"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgp_bernoulli_natgrads_xi.log_likelihood().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sufficiently small steps, it shouldn't make a difference which transform is used, but for large \n",
    "steps this can make a difference in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
