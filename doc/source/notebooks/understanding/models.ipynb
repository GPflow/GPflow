{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating GPflow models\n",
    "\n",
    "One of the key ingredients in GPflow is the model class, which enables you to carefully control parameters. This notebook shows how some of these parameter control features work, and how to build your own model with GPflow. First we'll look at:\n",
    "\n",
    " - how to view models and parameters\n",
    " - how to set parameter values\n",
    " - how to constrain parameters (for example, variance > 0)\n",
    " - how to fix model parameters\n",
    " - how to apply priors to parameters\n",
    " - how to optimise models\n",
    "\n",
    "Then we'll show how to build a simple logistic regression model, demonstrating the ease of the parameter framework. \n",
    "\n",
    "GPy users should feel right at home, but there are some small differences.\n",
    "\n",
    "First, let's deal with the usual notebook boilerplate and make a simple GP regression model. See [Basic (Gaussian likelihood) GP regression model](../basics/regression.ipynb) for specifics of the model; we just want some parameters to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gpflow\n",
    "import tensorflow_probability as tfp\n",
    "from gpflow.config import to_default_float\n",
    "from gpflow.utilities import print_summary, set_trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by creating a very simple GP regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate toy data\n",
    "np.random.seed(1)\n",
    "X = np.random.rand(20, 1)\n",
    "Y = np.sin(12 * X) + 0.66 * np.cos(25 * X) + np.random.randn(20,1) * 0.01\n",
    "\n",
    "m = gpflow.models.GPR((X, Y), kernel=gpflow.kernels.Matern32() + gpflow.kernels.Linear())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing, getting, and setting parameters\n",
    "You can display the state of the model in a terminal by using `print_summary(m)`. You can change the display format using the `fmt` keyword argument, e.g. `'html'`. In a notebook, you can also use `fmt='notebook'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                             </th><th>class    </th><th>transform  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th style=\"text-align: right;\">  value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.kernels[0].variance   </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">      1</td></tr>\n",
       "<tr><td>GPR.kernel.kernels[0].lengthscale</td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">      1</td></tr>\n",
       "<tr><td>GPR.kernel.kernels[1].variance   </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">      1</td></tr>\n",
       "<tr><td>GPR.likelihood.variance          </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">      1</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_summary(m, fmt='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has four parameters. The kernel is made of the sum of two parts. The first (counting from zero) is a Matern32 kernel that has a variance parameter and a lengthscale parameter; the second is a linear kernel that has only a variance parameter. There is also a parameter that controls the variance of the noise, as part of the likelihood. \n",
    "\n",
    "All the model variables have been initialised at `1.0`. You can access individual parameters in the same way that you display the state of the model in a terminal; for example, to see all the parameters that are part of the likelihood, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name             </th><th>class    </th><th>transform  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th style=\"text-align: right;\">  value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Gaussian.variance</td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">      1</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_summary(m.likelihood, fmt='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets more useful with more complex models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the value of a parameter, just use `assign()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                             </th><th>class    </th><th>transform  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th style=\"text-align: right;\">  value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.kernels[0].variance   </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   1   </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[0].lengthscale</td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   0.5 </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[1].variance   </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   1   </td></tr>\n",
       "<tr><td>GPR.likelihood.variance          </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   0.01</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.kernel.kernels[0].lengthscale.assign(0.5)\n",
    "m.likelihood.variance.assign(0.01)\n",
    "print_summary(m, fmt='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints and trainable variables\n",
    "\n",
    "GPflow helpfully creates an unconstrained representation of all the variables. In the previous example, all the variables are constrained positively (see the **transform** column in the table); the unconstrained representation is given by $\\alpha = \\log(\\exp(\\theta)-1)$. The `trainable_parameters` property returns the constrained values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=153, shape=(), dtype=float64, numpy=0.5>,\n",
       " <tf.Tensor: id=155, shape=(), dtype=float64, numpy=1.0>,\n",
       " <tf.Tensor: id=157, shape=(), dtype=float64, numpy=1.0>,\n",
       " <tf.Tensor: id=159, shape=(), dtype=float64, numpy=0.009999999776482648>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.trainable_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parameter has an `unconstrained_variable` attribute that enables you to access the unconstrained value as a TensorFlow `Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float64, numpy=-0.43275212956718856>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = m.kernel.kernels[0].lengthscale\n",
    "p.unconstrained_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the unconstrained value as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=161, shape=(), dtype=float64, numpy=-0.43275212956718856>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.transform.inverse(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraints are handled by the Bijector classes from the `tensorflow_probability` package. You might prefer to use the constraint $\\alpha = \\log(\\theta)$; this is easily done by replacing the parameter with one that has a different `transform` attribute (here we make sure to copy all other attributes across from the old parameter; this is not necessary when there is no `prior` and the `trainable` state is still the default of `True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_parameter = m.kernel.kernels[0].lengthscale\n",
    "new_parameter = gpflow.Parameter(old_parameter.value(),\n",
    "                                 trainable=old_parameter.trainable,\n",
    "                                 prior=old_parameter.prior,\n",
    "                                 name=old_parameter.name.split(':')[0],  # tensorflow is weird and adds ':0' to the name\n",
    "                                 transform=tfp.bijectors.Exp())\n",
    "m.kernel.kernels[0].lengthscale = new_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the lengthscale itself remains the same, the unconstrained lengthscale has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=173, shape=(), dtype=float64, numpy=-0.43275212956718856>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.transform.inverse(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also change the `transform` attribute in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.kernel.kernels[0].variance.transform = tfp.bijectors.Exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                             </th><th>class    </th><th>transform  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th style=\"text-align: right;\">  value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.kernels[0].variance   </td><td>Parameter</td><td>Exp        </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   1   </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[0].lengthscale</td><td>Parameter</td><td>Exp        </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   0.5 </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[1].variance   </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   1   </td></tr>\n",
       "<tr><td>GPR.likelihood.variance          </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   0.01</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_summary(m, fmt='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing whether a parameter will be trained in optimisation\n",
    "\n",
    "Another helpful feature is the ability to fix parameters. To do this, simply set the `trainable` attribute to `False`; this is shown in the **trainable** column of the representation, and the corresponding variable is removed from the free state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                             </th><th>class    </th><th>transform  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th style=\"text-align: right;\">  value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.kernels[0].variance   </td><td>Parameter</td><td>Exp        </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   1   </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[0].lengthscale</td><td>Parameter</td><td>Exp        </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   0.5 </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[1].variance   </td><td>Parameter</td><td>Softplus   </td><td>False      </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   1   </td></tr>\n",
       "<tr><td>GPR.likelihood.variance          </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   0.01</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.kernel.kernels[1].variance.trainable = False\n",
    "print_summary(m, fmt='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=196, shape=(), dtype=float64, numpy=0.5>,\n",
       " <tf.Tensor: id=198, shape=(), dtype=float64, numpy=1.0>,\n",
       " <tf.Tensor: id=200, shape=(), dtype=float64, numpy=0.009999999776482648>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.trainable_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To unfix a parameter, just set the `trainable` attribute to `True` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                             </th><th>class    </th><th>transform  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th style=\"text-align: right;\">  value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.kernels[0].variance   </td><td>Parameter</td><td>Exp        </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   1   </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[0].lengthscale</td><td>Parameter</td><td>Exp        </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   0.5 </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[1].variance   </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   1   </td></tr>\n",
       "<tr><td>GPR.likelihood.variance          </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   0.01</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.kernel.kernels[1].variance.trainable = True\n",
    "print_summary(m, fmt='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you want to recursively change the `trainable` status of an object that *contains* parameters, you **must** use the `set_trainable()` utility function.\n",
    "\n",
    "A module (e.g. a model, kernel, likelihood, ... instance) does not have a `trainable` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sum' object has no attribute 'trainable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b12817422e2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sum' object has no attribute 'trainable'"
     ]
    }
   ],
   "source": [
    "m.kernel.trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                             </th><th>class    </th><th>transform  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th style=\"text-align: right;\">  value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.kernels[0].variance   </td><td>Parameter</td><td>Exp        </td><td>False      </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   1   </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[0].lengthscale</td><td>Parameter</td><td>Exp        </td><td>False      </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   0.5 </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[1].variance   </td><td>Parameter</td><td>Softplus   </td><td>False      </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   1   </td></tr>\n",
       "<tr><td>GPR.likelihood.variance          </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   0.01</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_trainable(m.kernel, False)\n",
    "print_summary(m, fmt='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priors\n",
    "\n",
    "You can set priors in the same way as transforms and trainability, by using `tensorflow_probability` distribution objects. Let's set a Gamma prior on the variance of the Matern32 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = gpflow.kernels.Matern32()\n",
    "k.variance.prior = tfp.distributions.Gamma(\n",
    "    to_default_float(2), to_default_float(3))\n",
    "\n",
    "print_summary(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                             </th><th>class    </th><th>transform  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th style=\"text-align: right;\">  value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.kernels[0].variance   </td><td>Parameter</td><td>Exp        </td><td>False      </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   1   </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[0].lengthscale</td><td>Parameter</td><td>Exp        </td><td>False      </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   0.5 </td></tr>\n",
       "<tr><td>GPR.kernel.kernels[1].variance   </td><td>Parameter</td><td>Softplus   </td><td>False      </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   1   </td></tr>\n",
       "<tr><td>GPR.likelihood.variance          </td><td>Parameter</td><td>Softplus   </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">   0.01</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.kernel.kernels[0].variance.prior = tfp.distributions.Gamma(\n",
    "    to_default_float(2), to_default_float(3))\n",
    "print_summary(m, fmt='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "\n",
    "To optimise your model, first create an instance of an optimiser (in this case, `gpflow.train.ScipyOptimizer`), which has optional arguments that are passed to `scipy.optimize.minimize` (we minimise the negative log likelihood). Then, call the `minimize` method of that optimiser, with your model as the optimisation target. Variables that have priors are maximum a priori (MAP) estimated, that is, we add the log prior to the log likelihood, and otherwise use Maximum Likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile()\n",
    "opt = gpflow.train.ScipyOptimizer()\n",
    "opt.minimize(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building new models\n",
    "\n",
    "To build new models, you'll need to inherit from `gpflow.models.Model`. Parameters are instantiated with `gpflow.Param`. You might also be interested in `gpflow.params.Parameterized`, which acts as a 'container' for `Param`s (for example, kernels are parameterised). \n",
    "\n",
    "In this very simple demo, we'll implement linear multiclass classification.\n",
    "\n",
    "There are two parameters: a weight matrix and a bias (offset). The key thing to implement is the private `_build_likelihood` method, which returns a TensorFlow scalar that represents the (log) likelihood. You can use param objects inside `_build_likelihood`, but you need to use the constrained tensor attribute to access the tensor to use when building the graph (for example, `self.kernel.variance.constrained_tensor`).\n",
    "\n",
    "Alternatively, decorate the function with `@gpflow.params_as_tensors` so that the objects appear as constrained tensors (for example, you can now refer to the TensorFlow object as `self.kernel.variance` rather than `self.kernel.variance.constrained_tensor`). This is useful when you are writing the model likelihood and dealing with the `constrained_tensor` attributes of several different `Param`s, but note that you cannot access any of the other features of a `Param` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class LinearMulticlass(gpflow.models.Model):\n",
    "    def __init__(self, X, Y, name=None):\n",
    "        super().__init__(name=name) # always call the parent constructor\n",
    "        \n",
    "        self.X = X.copy() # X is a numpy array of inputs\n",
    "        self.Y = Y.copy() # Y is a 1-of-k (one-hot) representation of the labels\n",
    "        \n",
    "        self.num_data, self.input_dim = X.shape\n",
    "        _, self.num_classes = Y.shape\n",
    "        \n",
    "        #make some parameters\n",
    "        self.W = gpflow.Param(np.random.randn(self.input_dim, self.num_classes))\n",
    "        self.b = gpflow.Param(np.random.randn(self.num_classes))\n",
    "       \n",
    "        # ^^ You must make the parameters attributes of the class for\n",
    "        # them to be picked up by the model. i.e. this won't work:\n",
    "        #\n",
    "        # W = gpflow.Param(...    <-- must be self.W\n",
    "    \n",
    "    @gpflow.params_as_tensors\n",
    "    def _build_likelihood(self): # takes no arguments\n",
    "        p = tf.nn.softmax(tf.matmul(self.X, self.W) + self.b) # Param variables are used as tensorflow arrays. \n",
    "        return tf.reduce_sum(tf.log(p) * self.Y) # be sure to return a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and that's it. Let's build a really simple demo to show that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "X = np.vstack([np.random.randn(10,2) + [2,2],\n",
    "               np.random.randn(10,2) + [-2,2],\n",
    "               np.random.randn(10,2) + [2,-2]])\n",
    "Y = np.repeat(np.eye(3), 10, 0)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (12,6)\n",
    "plt.scatter(X[:,0], X[:,1], 100, np.argmax(Y, 1), lw=2, cmap=plt.cm.viridis);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LinearMulticlass(X, Y)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = gpflow.train.ScipyOptimizer()\n",
    "opt.minimize(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-4:4:200j, -4:4:200j]\n",
    "X_test = np.vstack([xx.flatten(), yy.flatten()]).T\n",
    "f_test = np.dot(X_test, m.W.read_value()) + m.b.read_value()\n",
    "p_test = np.exp(f_test)\n",
    "p_test /= p_test.sum(1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(3):\n",
    "    plt.contour(xx, yy, p_test[:,i].reshape(200,200), [0.5], colors='k', linewidths=1)\n",
    "plt.scatter(X[:,0], X[:,1], 100, np.argmax(Y, 1), lw=2, cmap=plt.cm.viridis);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes the new model example and this notebook. You might want to see for yourself that the `LinearMulticlass` model and its parameters have all the functionality demonstrated here. You could also add some priors and run Hamiltonian Monte Carlo using the HMC optimiser `gpflow.train.HMC` and its `sample` method. See [Markov Chain Monte Carlo (MCMC)](../advanced/mcmc.ipynb) for more information on running the sampler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
