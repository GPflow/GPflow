{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of GPflow\n",
    "--\n",
    "*James Hensman, 2016*\n",
    "*Small edits Alexander G. de G. Matthews 2017*\n",
    "\n",
    "In this document I'll try to give some insight into what's happening under the hood in GPflow. First I'll explain how the hierarchical structure works: GPflow models, kernels, likelihoods and parameters are arranged in a tree. Next I'll attempt to explain what happens when we run `_compile`, which builds a tensorflow graph ready for execution. Lots of these ideas are taken from GPy, where I was strongly influenced by Max Zwiesselle's work. You can find some of that in the [paramz project](http://github.com/sods/paramz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree structure\n",
    "GPflow has some core classes. They are\n",
    "- The `Param` class (`GPflow.param.Param`) which is used to represent parameters.\n",
    "\n",
    "- The DataHolder class (`GPflow.data_holders.DataDict`) which is used similarly to represent data.\n",
    "\n",
    "- The `Parameterized` class (`GPflow.param.Parameterized`) which is a container for `Param`s. Subclasses include\n",
    "\n",
    "  - Kernels (`GPflow.kernels.Kern`)\n",
    "  \n",
    "  - Likelihoods (`GPflow.likelihoods.Likelihood`)\n",
    "  \n",
    "  - Mean functions (`GPflow.mean_functions.MeanFunction`)\n",
    "  \n",
    "- The model class (`GPflow.model.Model`) which, which is a special class of `Parameterized`. \n",
    "\n",
    "\n",
    "Together these form a tree: the model class is the trunk, `Parameterized` classes are on branches, and `Param` and `DataHolder` classes are at the leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It possible to have as many layers (branches) in this structure as you want. Here's a silly example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.p.p.p.p.\u001b[1mfoo\u001b[0m transform:(none) prior:None\n",
      "[  7.  12.]\n"
     ]
    }
   ],
   "source": [
    "m = GPflow.model.Model()\n",
    "m.p = GPflow.param.Parameterized()\n",
    "m.p.p = GPflow.param.Parameterized()\n",
    "m.p.p.p = GPflow.param.Parameterized()\n",
    "m.p.p.p.p = GPflow.param.Parameterized()\n",
    "m.p.p.p.p.foo = GPflow.param.Param(np.array([7, 12]))\n",
    "print m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going down the tree\n",
    "The tree is implicitely defined by the attributes of each class. To find all the child nodes of a node, we just search through the `__dict__` of the node, looking for `Parameterized` or `Param` objects. There's a built-in `sorted_params` property, which does this, and ensures that they always apear in the same order. A good example of a function that makes use of this is the `build_prior` function. In `Parameterized` this just calls `build_prior` on all the child nodes. In a `Param` object, this computes the log prior density, if applicable (else 0). Another good example that we'll see shortly is the `make_tf_array` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going up the tree\n",
    "All the elements of this tree inherrit from `Parentable`. This base class doesn't do much, apart from point 'up' the tree. Each element in the tree has a `_parent` attribute, which points at the member in the layer above. It's only possible to have one parent: this is a tree. The model (trunk) has a `None` parent. It's therefore possible to get a pointer to the trunk by recursing the `_parent` attribute, which is how we have defined `highest_parent`. This is used to tell the model if something has changed (like if a parameter has been swwitched to `fixed`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens in `compile()` ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "To understand the ideas behind the stucture of GPflow, let's first have a look at how to compute things in tensorflow.  Tensorflow works by building a symbolic graph, and then executing the graph using `session().run(graph)`. Data are fed into the graph using a dictionary in the form `session().run(graph, feed_dict={X_tf:X_np}`, where `X_tf` is a tensorflow placeholder, and `X_np` is a numpy array containing the data to be fed in. \n",
    "\n",
    "Let's have a look at a simple Linear regression example, without any GPlow machinery. We'll write a build_likelihood() function which will construct the tensorflow graph, and then we'll feed the graph with some data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1604.31329751] \n",
      "\n",
      "[[-463.42971664]\n",
      " [-472.88070167]] \n",
      "\n",
      "[ 8661.42009544]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# generate a dataset\n",
    "X = np.random.randn(10, 2)\n",
    "Y = np.dot(X, np.array([2.3, -1.2])) + np.random.randn(10, 1) * 0.2\n",
    "\n",
    "# placeholders for the data and coefficients\n",
    "X_tf = tf.placeholder(tf.float64)\n",
    "Y_tf = tf.placeholder(tf.float64)\n",
    "coeffs_tf = tf.placeholder(tf.float64)\n",
    "noise_var_tf = tf.placeholder(tf.float64)\n",
    "\n",
    "def build_likelihood():\n",
    "    err = Y_tf - tf.matmul(X_tf, coeffs_tf)\n",
    "    log_likelihood = -0.5*tf.log(noise_var_tf) -0.5 / noise_var_tf * tf.reduce_sum(tf.square(err))\n",
    "    return log_likelihood\n",
    "\n",
    "ll_graph = build_likelihood()\n",
    "grads_tf = tf.gradients(ll_graph, [coeffs_tf, noise_var_tf])\n",
    "sess = tf.InteractiveSession()\n",
    "coeffs_guess = np.random.randn(2, 1)\n",
    "noise_guess = np.random.rand(1)\n",
    "log_lik, grad_coeff, grad_noise = sess.run([ll_graph]+grads_tf, feed_dict={X_tf:X,\n",
    "                                                             Y_tf:Y,\n",
    "                                                             coeffs_tf:coeffs_guess,\n",
    "                                                             noise_var_tf:noise_guess})\n",
    "print log_lik, '\\n\\n', grad_coeff, '\\n\\n', grad_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite a lot of code for such a simple task! Here's the same code using GPflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling tensorflow function...\n",
      "done\n",
      "[array([ 1603.25150772]), array([ -321.7506682 ,  -594.06003423, -1603.55961173])]\n"
     ]
    }
   ],
   "source": [
    "import GPflow\n",
    "\n",
    "class LinReg(GPflow.model.Model):\n",
    "    \n",
    "    def __init__(self, X, Y):\n",
    "        GPflow.model.Model.__init__(self)\n",
    "        self.coeffs = GPflow.param.Param(np.random.randn(2, 1)) # Param contains initial guess at value\n",
    "        self.noise_var = GPflow.param.Param(np.random.rand(), transform=GPflow.transforms.Exp())\n",
    "        self.X, self.Y = GPflow.param.DataHolder(X), GPflow.param.DataHolder(Y)\n",
    "        \n",
    "    def build_likelihood(self):\n",
    "        err = self.Y - tf.matmul(self.X, self.coeffs)  # we can use self.coeffs as if it were a tf object\n",
    "        log_likelihood = -0.5*tf.log(self.noise_var) -0.5 / self.noise_var * tf.reduce_sum(tf.square(err))\n",
    "        return log_likelihood\n",
    "    \n",
    "m = LinReg(X, Y)\n",
    "m._compile()\n",
    "print m._objective(m.get_free_state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sidenote: that that's not the recommended way to get the log likelihood out of the model. Use `m.compute_log_likelihood()` instead. \n",
    "\n",
    "That's not a huge saving in code, but the savings mount up for more complex models. Here's what happens in the above code. \n",
    "\n",
    "1. When the Model is instantiated (`m = LinReg(X, Y)`), the constructor `__init__` is called. In this function:\n",
    "  - `Model.__init__` creates a general purpose model. This contains its own `Session`, a placeholder `m._free_vars` for all our parameters, and a some convenient functions for e.g. optimization.  \n",
    "  - We define a `Param` object `self.coeffs` for the coefficients, and `self.noise_var` for the noise variance. This lets us pass the current guess at the parameter into `model._session`. The current state of each parameter is stored as `self._array`; we have initialized each parameter with a guess. \n",
    "  - We store the data as `DataHolders`, which allows us to pass the data into `m._session`.\n",
    "  \n",
    "2. When we manually compile the model (`m._compile`), the models calls the build_likelihod function. But, in order to let us use the parameters and dataholders as a tensorflow object (see e.g. `tf.matmul(self.X, self.coeffs)`), this is done inside a python 'context' that we call `tf_mode`. Before we can use `tf_mode`, we must first call `make_tf_array`.\n",
    "\n",
    "3. `make_tf_array` takes the tensorflow placeholder `m._free_vars` and distributes it amongst the parameters. We go through all the parameters in turn, and take a chunk of `_free_vars` of the correct length, and reshape it (and possibly transform it, see later) into a tensorflow variable that can represent the parameter in the graph. In our simple example, `_free_vars` has length three. The first two elements are transformed into a 2x1 matrix of coeffs, and the last one is exponentiated to become the noise variance. Each Param object stores its transformed, reshaped section of `_free_vars` as `self._tf_array`. Similiarly, the DataHolder objects hold placeholders into which we later feed the data as `self._tf_array`. \n",
    "\n",
    "4. Inside the clause `with m.tf_mode()`, all of the parameters (and dataholders) are replaced with their `_tf_array` attributes. The `_compile` function can then call the `build_likelhood` function which builds a representation of the likelihood as a function of `_free_vars` (via the `_tf_array` attributes). It also builds a convenient function `self._objective` which allows us to link up with scipy's `optimize` routines. \n",
    "\n",
    "5. The `_objective` function is a wrapper around a call to the model's `_session.run`. It calls `session.run()`, requesting the negative log_likelihood and the gradients with respect to `_free_vars`. Into the feed_dict, it passes the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(2, 1), dtype=float64)\n",
      "Tensor(\"Reshape_1:0\", shape=(1,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print m.coeffs._tf_array\n",
    "print m.noise_var._tf_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_feed_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
