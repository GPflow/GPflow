{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural gradients\n",
    "\n",
    "This shows some basic usage of the natural gradient optimizer, both on its own and in combination with other optimizers using the Actions class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import gpflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural gradients turn VGP into GPR in a single step, if the likelihood is Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/sml/hrs13/GPflow/gpflow/densities.py:89: UserWarning: Shape of x must be 2D at computation.\n",
      "  warnings.warn('Shape of x must be 2D at computation.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact GP likelihood: -254.9348\n",
      "VGP likelihood is before nat grad step: -1404.0805\n",
      "VGP likelihood after a single nat grad step: -254.9348\n"
     ]
    }
   ],
   "source": [
    "N, D = 100, 2\n",
    "np.random.seed(0)\n",
    "X = np.random.uniform(size=(N, D))\n",
    "Y = np.sin(10*X)\n",
    "\n",
    "make_kern = lambda : gpflow.kernels.Matern52(D)\n",
    "\n",
    "m_vgp = gpflow.models.VGP(X, Y, make_kern(), gpflow.likelihoods.Gaussian())\n",
    "m_gpr = gpflow.models.GPR(X, Y, make_kern())\n",
    "\n",
    "for model in m_vgp, m_gpr:\n",
    "    model.likelihood.variance = 0.1\n",
    "    \n",
    "print('exact GP likelihood: {:.4f}'.format(m_gpr.compute_log_likelihood()))\n",
    "\n",
    "print('VGP likelihood is before nat grad step: {:.4f}'.format(m_vgp.compute_log_likelihood()))\n",
    "\n",
    "natgrad_optimizer = gpflow.training.NatGradOptimizer(gamma=1.)\n",
    "\n",
    "natgrad_optimizer.minimize(m_vgp, maxiter=1, var_list=[[m_vgp.q_mu, m_vgp.q_sqrt]])\n",
    "\n",
    "# should be the same as GP likelihoo (up to discrepanceies caused by jitter etc)\n",
    "print('VGP likelihood after a single nat grad step: {:.4f}'.format(m_vgp.compute_log_likelihood()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaving an ordinary gradient step with a nat grad step\n",
    "In this case (Gaussian likelihood) it achieves optimization of hyperparameters as if the model were GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPR with Adam: iteration 0 likelihood -252.4454\n",
      "GPR with Adam: iteration 1 likelihood -249.9916\n",
      "GPR with Adam: iteration 2 likelihood -247.5735\n",
      "GPR with Adam: iteration 3 likelihood -245.1913\n",
      "GPR with Adam: iteration 4 likelihood -242.8452\n",
      "VGP with nat grads with Adam: iteration 0 likelihood -252.4454\n",
      "VGP with nat grads with Adam: iteration 1 likelihood -249.9916\n",
      "VGP with nat grads with Adam: iteration 2 likelihood -247.5735\n",
      "VGP with nat grads with Adam: iteration 3 likelihood -245.1913\n",
      "VGP with nat grads with Adam: iteration 4 likelihood -242.8452\n",
      "GPR lengthscale: 0.9687\n",
      "VGP lengthscale: 0.9687\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "iterations = 5\n",
    "\n",
    "# use Adam on GPR. We'll add a callback too, so we can see what the value is at each iteration\n",
    "class PrintAction(gpflow.actions.Action):\n",
    "    def __init__(self, model, text):\n",
    "        self.model = model\n",
    "        self.text = text\n",
    "    def run(self, ctx):\n",
    "        l = ctx.session.run(self.model.likelihood_tensor)\n",
    "        print('{}: iteration {} likelihood {:.4f}'.format(self.text, ctx.iteration, l))\n",
    "\n",
    "def run_adam_with_callback(model, lr, iterations):\n",
    "    adam_tensor = gpflow.training.AdamOptimizer(lr).make_optimize_tensor(model)\n",
    "    adam_op = gpflow.actions.Optimization().with_optimizer_tensor(adam_tensor).with_run_kwargs()\n",
    "    cb_op = PrintAction(model, 'GPR with Adam')\n",
    "    ops = gpflow.actions.Group(adam_op, cb_op)\n",
    "    gpflow.actions.Loop(ops).with_settings(stop=iterations)()\n",
    "    model.anchor(model.enquire_session())\n",
    "\n",
    "run_adam_with_callback(m_gpr, lr, iterations)\n",
    "\n",
    "# use Adam + nat grads on VGP. The hyperparameters at the end should match the GPR model\n",
    "def run_nat_grads_with_adam(model, lr, gamma, iterations, var_list=None, callback=None):\n",
    "    \n",
    "    # we'll make use of this later when we use a XiTransform\n",
    "    if var_list is None:\n",
    "        var_list = [[model.q_mu, model.q_sqrt]]\n",
    "\n",
    "    # we don't want adam optimizing these\n",
    "    model.q_mu.set_trainable(False)\n",
    "    model.q_sqrt.set_trainable(False)\n",
    "\n",
    "    # the two optimizers we'll be combining \n",
    "    adam_opt = gpflow.training.AdamOptimizer(lr)\n",
    "    ng_opt = gpflow.training.NatGradOptimizer(gamma)\n",
    "    \n",
    "    # the tensorflow operations\n",
    "    t1 = adam_opt.make_optimize_tensor(model)\n",
    "    t2 = ng_opt.make_optimize_tensor(model, var_list=var_list)\n",
    "\n",
    "    # make the actions\n",
    "    op1 = gpflow.actions.Optimization().with_optimizer_tensor(t1).with_run_kwargs()\n",
    "    op2 = gpflow.actions.Optimization().with_optimizer_tensor(t2).with_run_kwargs()\n",
    "    if callback is not None:\n",
    "        ops = gpflow.actions.Group(op1, op2, callback) \n",
    "    else:\n",
    "        ops = gpflow.actions.Group(op1, op2)\n",
    "    \n",
    "    # run the loop\n",
    "    gpflow.actions.Loop(ops).with_settings(stop=iterations)()\n",
    "\n",
    "    model.anchor(model.enquire_session())\n",
    "\n",
    "# compare\n",
    "run_nat_grads_with_adam(m_vgp, lr, 1., iterations, callback=PrintAction(m_vgp, 'VGP with nat grads with Adam'))\n",
    "\n",
    "print('GPR lengthscale: {:.4f}'.format(m_gpr.kern.lengthscales.read_value()))\n",
    "print('VGP lengthscale: {:.4f}'.format(m_vgp.kern.lengthscales.read_value()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This also works for the sparse model\n",
    "Nat grads turn SVGP into SGPR in the Gaussian likelihood case. We can apply the above with hyperparameters, too, though here we'll just do a single step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "Z = np.random.uniform(size=(M, D))\n",
    "\n",
    "m_svgp = gpflow.models.SVGP(X, Y, make_kern(), gpflow.likelihoods.Gaussian(), Z=Z)\n",
    "m_sgpr = gpflow.models.SGPR(X, Y, make_kern(), Z=Z)\n",
    "\n",
    "for model in m_svgp, m_sgpr:\n",
    "    model.likelihood.variance = 0.1\n",
    "    \n",
    "print('analytically optimal sparse model likelihood: {:.4f}'.format(m_sgpr.compute_log_likelihood()))\n",
    "\n",
    "print('SVGP likelihood before nat grad step: {:.4f}'.format(m_svgp.compute_log_likelihood()))\n",
    "\n",
    "natgrad_optimizer.minimize(m_svgp, maxiter=1, var_list=[[m_svgp.q_mu, m_svgp.q_sqrt]])\n",
    "\n",
    "# should be the same as GP likelihoo (up to discrepanceies caused by jitter etc)\n",
    "print('SVGP likelihood after a single nat grad step: {:.4f}'.format(m_svgp.compute_log_likelihood()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatches\n",
    "A crucial property of the natural gradient method is that it still works with minibatches. We need to use a smaller gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_svgp_minibatch = gpflow.models.SVGP(X, Y, make_kern(), gpflow.likelihoods.Gaussian(), \n",
    "                                      Z=Z, minibatch_size=50)\n",
    "m_svgp_minibatch.likelihood.variance = 0.1\n",
    "\n",
    "natgrad_optimizer_minibatch = gpflow.training.NatGradOptimizer(gamma=0.1)\n",
    "\n",
    "natgrad_optimizer_minibatch.minimize(m_svgp_minibatch, \n",
    "                                     maxiter=100, \n",
    "                                     var_list=[[m_svgp_minibatch.q_mu, m_svgp_minibatch.q_sqrt]])\n",
    "\n",
    "L = np.average([m_svgp_minibatch.compute_log_likelihood() for _ in range(1000)])\n",
    "print('minibatch SVGP likelihood after nat grad optimization: {:.4f}'.format(L))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with ordinary gradients in the non-conjugate case\n",
    "\n",
    "#### (Take home message: natural gradients are always better)\n",
    "Compared with doing SVGP with ordinary gradients with minibatches, the nat grads optimizer is much faster in the Gaussian case. \n",
    "\n",
    "Here we'll do hyperparameter learning together optimization of the variational parameters, comparing the interleaved nat grad approach and using ordinary gradients for the hyperparameters and variational parameters jointly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_svgp_minibatch_ordinary = gpflow.models.SVGP(X, Y, make_kern(), gpflow.likelihoods.Gaussian(), \n",
    "                                               Z=Z, minibatch_size=50)\n",
    "\n",
    "m_svgp_minibatch_nat = gpflow.models.SVGP(X, Y, make_kern(), gpflow.likelihoods.Gaussian(), \n",
    "                                          Z=Z, minibatch_size=50)\n",
    "\n",
    "# ordinary gradients and Adam\n",
    "gpflow.training.AdamOptimizer(lr).minimize(m_svgp_minibatch_ordinary, maxiter=iterations)\n",
    "\n",
    "# nat grads with Adam \n",
    "run_nat_grads_with_adam(m_svgp_minibatch_nat, lr, 0.1, iterations)\n",
    "\n",
    "L = np.average([m_svgp_minibatch_ordinary.compute_log_likelihood() for _ in range(1000)])\n",
    "print('ordinary grads SVGP likelihood: {:.4f}'.format(L))\n",
    "\n",
    "L = np.average([m_svgp_minibatch_nat.compute_log_likelihood() for _ in range(1000)])\n",
    "print('nat grads + Adam SVGP likelihood: {:.4f}'.format(L))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with ordinary gradients in the non-conjugate case\n",
    "\n",
    "#### (Take home message: natural gradients are usually better)\n",
    "\n",
    "We can use nat grads even when the likelihood isn't Gaussian. It isn't guaranteed to be better, but it usually is better in practical situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_binary = np.random.choice([1., -1], size=X.shape)\n",
    "\n",
    "m_vgp_bernoulli = gpflow.models.VGP(X, Y_binary, make_kern(), gpflow.likelihoods.Bernoulli())\n",
    "m_vgp_bernoulli_natgrads = gpflow.models.VGP(X, Y_binary, make_kern(), gpflow.likelihoods.Bernoulli())\n",
    "\n",
    "# ordinary gradients and Adam\n",
    "gpflow.training.AdamOptimizer(lr).minimize(m_vgp_bernoulli, maxiter=iterations)\n",
    "\n",
    "# nat grads with Adam \n",
    "run_nat_grads_with_adam(m_vgp_bernoulli_natgrads, lr, 0.1, iterations)\n",
    "\n",
    "print('ordinary grads SVGP likelihood: {:.4f}'.format(m_vgp_bernoulli.compute_log_likelihood()))\n",
    "\n",
    "print('nat grads + Adam SVGP likelihood: {:.4f}'.format(m_vgp_bernoulli_natgrads.compute_log_likelihood()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose to run natural gradients in another parameterization. The \n",
    "sensible choice might is the model parameters (q_mu, q_sqrt), which is already in gpflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vgp_bernoulli_natgrads_xi = gpflow.models.VGP(X, Y_binary, make_kern(), gpflow.likelihoods.Bernoulli())\n",
    "\n",
    "var_list = [[m_vgp_bernoulli_natgrads_xi.q_mu, \n",
    "             m_vgp_bernoulli_natgrads_xi.q_sqrt, \n",
    "             gpflow.training.XiSqrtMeanVar()]]\n",
    "run_nat_grads_with_adam(m_vgp_bernoulli_natgrads_xi, lr, 0.01, iterations, var_list=var_list)\n",
    "\n",
    "print('nat grads + Adam with XiSqrtMeanVar: {:.4f}'.format(m_vgp_bernoulli_natgrads_xi.compute_log_likelihood()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sufficiently small steps, it shouldn't make a difference which transform is used, but for large \n",
    "step this can make a difference in practice.\n",
    "\n",
    "Further points:\n",
    "* It can help to start gamma small and increase (to about 0.1) during optimization \n",
    "* Natural gradients are especially useful for the gplvm model\n",
    "* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
