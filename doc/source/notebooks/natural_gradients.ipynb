{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural gradients\n",
    "\n",
    "This shows some basic usage of the natural gradient optimizer, both on its own and in combination with other optimizers using the Actions class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import gpflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural gradients turn VGP into GPR in a single step, if the likelihood is Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/sml/hrs13/GPflow/gpflow/densities.py:89: UserWarning: Shape of x must be 2D at computation.\n",
      "  warnings.warn('Shape of x must be 2D at computation.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact GP likelihood: -254.9348\n",
      "VGP likelihood is before nat grad step: -1404.0805\n",
      "VGP likelihood after a single nat grad step: -254.9348\n"
     ]
    }
   ],
   "source": [
    "N, D = 100, 2\n",
    "np.random.seed(0)\n",
    "X = np.random.uniform(size=(N, D))\n",
    "Y = np.sin(10*X)\n",
    "\n",
    "make_kern = lambda : gpflow.kernels.Matern52(D)\n",
    "\n",
    "m_vgp = gpflow.models.VGP(X, Y, make_kern(), gpflow.likelihoods.Gaussian())\n",
    "m_gpr = gpflow.models.GPR(X, Y, make_kern())\n",
    "\n",
    "for model in m_vgp, m_gpr:\n",
    "    model.likelihood.variance = 0.1\n",
    "    \n",
    "print('exact GP likelihood: {:.4f}'.format(m_gpr.compute_log_likelihood()))\n",
    "\n",
    "print('VGP likelihood is before nat grad step: {:.4f}'.format(m_vgp.compute_log_likelihood()))\n",
    "\n",
    "natgrad_optimizer = gpflow.training.NatGradOptimizer(gamma=1.)\n",
    "\n",
    "natgrad_optimizer.minimize(m_vgp, maxiter=1, var_list=[[m_vgp.q_mu, m_vgp.q_sqrt]])\n",
    "\n",
    "# should be the same as GP likelihoo (up to discrepanceies caused by jitter etc)\n",
    "print('VGP likelihood after a single nat grad step: {:.4f}'.format(m_vgp.compute_log_likelihood()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaving an ordinary gradient step with a nat grad step\n",
    "In this case (Gaussian likelihood) it achieves optimization of hyperparameters as if the model were GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPR lengthscale: 0.4827\n",
      "VGP lengthscale: 0.4827\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "iterations = 100\n",
    "\n",
    "# use Adam on GPR \n",
    "gpflow.training.AdamOptimizer(lr).minimize(m_gpr, maxiter=iterations)\n",
    "\n",
    "# use Adam + nat grads on VGP. The hyperparameters at the end should match the GPR model\n",
    "def run_nat_grads_with_adam(model, lr, gamma, iterations, var_list=None):\n",
    "    \n",
    "    # we'll make use of this later when we use a XiTransform\n",
    "    if var_list is None:\n",
    "        var_list = [[model.q_mu, model.q_sqrt]]\n",
    "\n",
    "    # we don't want adam optimizing these\n",
    "    model.q_mu.set_trainable(False)\n",
    "    model.q_sqrt.set_trainable(False)\n",
    "\n",
    "    # the two optimizers we'll be combining \n",
    "    adam_opt = gpflow.training.AdamOptimizer(lr)\n",
    "    ng_opt = gpflow.training.NatGradOptimizer(gamma)\n",
    "    \n",
    "    # the tensorflow operations\n",
    "    t1 = adam_opt.make_optimize_tensor(model)\n",
    "    t2 = ng_opt.make_optimize_tensor(model, var_list=var_list)\n",
    "\n",
    "    # make the actions\n",
    "    op1 = gpflow.actions.Optimization().with_optimizer_tensor(t1).with_run_kwargs()\n",
    "    op2 = gpflow.actions.Optimization().with_optimizer_tensor(t2).with_run_kwargs()\n",
    "    ops = gpflow.actions.Group(op1, op2)\n",
    "    \n",
    "    # run the loop\n",
    "    gpflow.actions.Loop(ops).with_settings(stop=iterations)()\n",
    "\n",
    "    model.anchor(model.enquire_session())\n",
    "\n",
    "# compare\n",
    "run_nat_grads_with_adam(m_vgp, lr, 1., iterations)\n",
    "\n",
    "print('GPR lengthscale: {:.4f}'.format(m_gpr.kern.lengthscales.read_value()))\n",
    "print('VGP lengthscale: {:.4f}'.format(m_vgp.kern.lengthscales.read_value()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This also works for the sparse model\n",
    "Nat grads turn SVGP into SGPR in the Gaussian likelihood case. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytically optimal sparse model likelihood: -281.5543\n",
      "SVGP likelihood before nat grad step: -1404.0805\n",
      "SVGP likelihood after a single nat grad step: -281.5543\n"
     ]
    }
   ],
   "source": [
    "M = 10\n",
    "Z = np.random.uniform(size=(M, D))\n",
    "\n",
    "m_svgp = gpflow.models.SVGP(X, Y, make_kern(), gpflow.likelihoods.Gaussian(), Z=Z)\n",
    "m_sgpr = gpflow.models.SGPR(X, Y, make_kern(), Z=Z)\n",
    "\n",
    "for model in m_svgp, m_sgpr:\n",
    "    model.likelihood.variance = 0.1\n",
    "    \n",
    "print('analytically optimal sparse model likelihood: {:.4f}'.format(m_sgpr.compute_log_likelihood()))\n",
    "\n",
    "print('SVGP likelihood before nat grad step: {:.4f}'.format(m_svgp.compute_log_likelihood()))\n",
    "\n",
    "natgrad_optimizer.minimize(m_svgp, maxiter=1, var_list=[[m_svgp.q_mu, m_svgp.q_sqrt]])\n",
    "\n",
    "# should be the same as GP likelihoo (up to discrepanceies caused by jitter etc)\n",
    "print('SVGP likelihood after a single nat grad step: {:.4f}'.format(m_svgp.compute_log_likelihood()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatches\n",
    "A crucial property of the natural gradient method is that it still works with minibatches. We need to use a smaller gamma, though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatch SVGP likelihood after nat grad optimization: -281.8539\n"
     ]
    }
   ],
   "source": [
    "m_svgp_minibatch = gpflow.models.SVGP(X, Y, make_kern(), gpflow.likelihoods.Gaussian(), \n",
    "                                      Z=Z, minibatch_size=50)\n",
    "m_svgp_minibatch.likelihood.variance = 0.1\n",
    "\n",
    "natgrad_optimizer_minibatch = gpflow.training.NatGradOptimizer(gamma=0.1)\n",
    "\n",
    "natgrad_optimizer_minibatch.minimize(m_svgp_minibatch, \n",
    "                                     maxiter=100, \n",
    "                                     var_list=[[m_svgp_minibatch.q_mu, m_svgp_minibatch.q_sqrt]])\n",
    "\n",
    "L = np.average([m_svgp_minibatch.compute_log_likelihood() for _ in range(1000)])\n",
    "print('minibatch SVGP likelihood after nat grad optimization: {:.4f}'.format(L))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with ordinary gradients in the non-conjugate case\n",
    "\n",
    "#### (Take Natural gradients are always better)\n",
    "Compared with doing SVGP with ordinary gradients with minibatches, nat grads is much faster, in the Gaussian case\n",
    "\n",
    "Here we'll do hyperparameter learning together optimization of the variational parameters, comparing the interleaved nat grad approach and using ordinary gradients for the hyperparameters and variational parameters jointly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_svgp_minibatch_ordinary = gpflow.models.SVGP(X, Y, make_kern(), gpflow.likelihoods.Gaussian(), \n",
    "                                               Z=Z, minibatch_size=50)\n",
    "\n",
    "m_svgp_minibatch_nat = gpflow.models.SVGP(X, Y, make_kern(), gpflow.likelihoods.Gaussian(), \n",
    "                                          Z=Z, minibatch_size=50)\n",
    "\n",
    "# ordinary gradients and Adam\n",
    "gpflow.training.AdamOptimizer(lr).minimize(m_svgp_minibatch_ordinary, maxiter=iterations)\n",
    "\n",
    "# nat grads with Adam \n",
    "run_nat_grads_with_adam(m_svgp_minibatch_nat, lr, 0.1, iterations)\n",
    "\n",
    "L = np.average([m_svgp_minibatch_ordinary.compute_log_likelihood() for _ in range(1000)])\n",
    "print('ordinary grads SVGP likelihood: {:.4f}'.format(L))\n",
    "\n",
    "L = np.average([m_svgp_minibatch_nat.compute_log_likelihood() for _ in range(1000)])\n",
    "print('nat grads + Adam SVGP likelihood: {:.4f}'.format(L))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with ordinary gradients in the non-conjugate case\n",
    "\n",
    "#### (Natural gradients are usually better)\n",
    "\n",
    "We can use nat grads even when the likelihood isn't Gaussian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_binary = np.random.choice([1., -1], size=X.shape)\n",
    "\n",
    "m_vgp_bernoulli = gpflow.models.VGP(X, Y_binary, make_kern(), gpflow.likelihoods.Bernoulli())\n",
    "m_vgp_bernoulli_natgrads = gpflow.models.VGP(X, Y_binary, make_kern(), gpflow.likelihoods.Bernoulli())\n",
    "\n",
    "# ordinary gradients and Adam\n",
    "gpflow.training.AdamOptimizer(lr).minimize(m_vgp_bernoulli, maxiter=iterations)\n",
    "\n",
    "# nat grads with Adam \n",
    "run_nat_grads_with_adam(m_vgp_bernoulli_natgrads, lr, 0.1, iterations)\n",
    "\n",
    "print('ordinary grads SVGP likelihood: {:.4f}'.format(m_vgp_bernoulli.compute_log_likelihood()))\n",
    "\n",
    "print('nat grads + Adam SVGP likelihood: {:.4f}'.format(m_vgp_bernoulli_natgrads.compute_log_likelihood()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose to run natural gradients in another parameterization. The \n",
    "sensible choice might is the model parameters (q_mu, q_sqrt), which is already in gpflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m_vgp_bernoulli_natgrads_xi = gpflow.models.VGP(X, Y_binary, make_kern(), gpflow.likelihoods.Bernoulli())\n",
    "\n",
    "var_list = [[m_vgp_bernoulli_natgrads_xi.q_mu, \n",
    "             m_vgp_bernoulli_natgrads_xi.q_sqrt, \n",
    "             gpflow.training.XiSqrtMeanVar()]]\n",
    "run_nat_grads_with_adam(m_vgp_bernoulli_natgrads_xi, lr, 0.01, iterations, var_list=var_list)\n",
    "\n",
    "print('nat grads + Adam with XiSqrtMeanVar: {:.4f}'.format(m_vgp_bernoulli_natgrads_xi.compute_log_likelihood()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sufficiently small steps, it shouldn't make a difference which transform is used, but for large \n",
    "step this can make a difference in practice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
